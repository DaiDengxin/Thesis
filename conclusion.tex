\chapter{Conclusion} 
\label{ch:conclusion}

This thesis presented methods towards cost-effective and performance-aware vision algorithms.  The primary focus was on  effective algorithms in terms of annotation cost, for which we presented two different approaches, one for efficient visual annotation and one for learning with unlabeled data.  In addition, we examined how to reduce the computational cost associated with the training and testing of vision algorithms in the context of learning using privileged information and metric learning.   Finally, we investigated how  performance-ware algorithms can be learned, and then be used to facilitate down-streamed applications. A more detailed look at the specific contributions are summarized below. 

\section{Contributions} 

In Chapter~\ref{ch:draw-and-tell}, \emph{Efficient Visual Annotation with Speech Recognition}, we proposed an efficient annotation method for semantic image segmentation by leveraging the power of speech recognition. In this method, we allow annotations to speak  objects's names while they draw strokes on them.  Object names are recognized by a combination of speech recognition and webly-supervised object recognition.  The drawn strokes are then converted to semantic heatmaps for the corresponding classes by ensemble interactive segmentation. Finally, an extension to the standard fully convolutional networks~\citep{Long_2015_CVPR} is made to accommodate the `weak' annotation. The method yields comparable results to the same CNNs architecture trained with standard  annotations of full segmentation masks, while being 10x faster. 
 
In Chapter~\ref{ch:ensemble}, \emph{Representation Learning with Unlabeled Data}, we developed a method to learn new feature representations by exploring the data distribution patterns of unlabeled data.  The leaning takes advantages of discriminative learning and ensemble learning to effectively exploit  the \emph{manifold-smoothness} assumption: surrogate classes are sampled from the manifold of data samples, on which discriminative learning is performed to extract the high-level knowledge; the noise of the sampled training training set is mitigated by ensemble learning. The learned discriminative classifiers are used to generate the new high-level feature representations. Experiments on eight datasets show that the learned representations are superior to the standard feature representations. 

In Chapter~\ref{ch:svmplus}, \emph{Fast Training Algorithms for SVM+}, we developed  efficient training algorithms to the standard approach SVM+.  New fomulations with fewer constraints are formulated in the dual domain, making it solvable efficiently by the SMO algorithm of one-class SVM.  Experiments show that our proposed algorithms achieve significant speed-ups to  the state-of-the-art solvers for  SVM+.
  
In Chapter~\ref{ch:mi}, \emph{Efficient Metric Computation via Imitation}, we developed a method to allow computationally cheap features to imitate better-performing but computationally more expensive features. We treat the problem as a  problem of transfer learning, where the neighborhood property from expensive features are quantified into manifold structures. The manifold structures are view-independent, and can then be transferred to the space of cheap features. Finally, a linear transformation of the cheap features is learned so that the manifold can be approximated as well as possible.  The leaned transformation significantly boosts the performance of cheap features while retaining their efficiency.  Experiments on multiple experiments validate the efficacy of the method. 
  
In Chapter~\ref{ch:forecasting}, \emph{Performance Prediction: Success or Fail?}, we predicted how likely texture synthesis methods succeed on particular texture samples. To the aim, we collected a dataset of $21, 302$ textures and performed $4$ standard synthesis methods on it. The synthesized results were annotated based on their visual quality in terms of three levels. A set of relevant features were defined in order to learn a regressor to predict the performance of the methods. Extensive experiments show that the performance of texture synthesis methods can be predicted accurately.  

In Chapter~\ref{ch:scale-aware}, \emph{Performance Prediction: Under-, Properly-, or Over-Processed?}, we presented a method to predict whether a segment from image segmentation methods is under-segmented, properly-segmented, or over-segmented.  The prediction is then used to re-align the tree structure of hierarchical image segmentation, so that the depth of a regions is better coupled with its scale.  The improved hierarchy improves the quality of the hierarchical segmentation representations.


In Chapter~\ref{ch:SR4VT}, \emph{Performance Evaluation: Useful for Other Vision Tasks?}, we extensively evaluated the usefulness of image super-resolution for four standard vision tasks. This work has formalized the conception that image super-resolution is helpful for other vision tasks when the input images are of low-resolution.  


\section{Perspectives} 

\textbf{Integration of vision and speech}. 
The recent years have witnessed great advances in Computer Vision,
Language Processing, and Speech Recognition, thanks to deep learning
and big data. This is evidenced not only by the surge of academic papers, but also by the world-wide industry interests. 
The convincing successes in these individual fields naturally raise
the potentials of further integration towards solutions to more general AI
problems. Much excellent work has been done to integrate vision and
language, resulting in a wide collection of successful applications
such as image/video captioning~\citep{show:tell:caption}, movie description
~\citep{movie:dataset}, and visual question answering~\citep{VQA}. However, the importance of integrating vision and
speech has not yet fairly explored.  The work in Chapter~\ref{ch:draw-and-tell} made a small step in exploring this
potential with deep learning techniques.  

As an example, a better integration of vision and speech can lead to voice-user interface to pictures.  
Voice-user interface (VUI) has become more commonplace, and people are increasingly taking advantage of its good values --- it is hands-free, eyes-free, and far more mobile. As we people in $21^{th}$ century communicate in pictures,  there is a strong need to have a VUI to talk to pictures in order to perform scene-relevant tasks, such as image tagging, story logging, and visual question answering. Also, a VUI to pictures can offer assistance to blind people, and dictate robotics to react to real visual scenes (situations) in a hand-free manner such as in clinical robotic surgery. See Figure~\ref{con:fig:app} for two of the applications. 
We hope this work can inspire more research effort in integrating vision and speech. 

\begin{figure}
$\begin{tabular}{cccccc}
\includegraphics[width=0.46\linewidth, height=50mm]{./figs/fig_description-crop.pdf} &
\includegraphics[width=0.46\linewidth, height=50mm]{./figs/fig_vqa-crop.pdf} \\
\text{\textbf{(a) Image Describing}} &  
\text{\textbf{(b) Visual Question Answering}} \\  
\end{tabular}$
\caption{Two potential applications of the voice-user interface to picture: (a) speech-based
  image describing and (b) speech-based visual question answering. The
  inputs are an image and an audio which carries descriptions or questions relevant to the scene in the image. The system learns jointly from two data modalities (image and audio) for the optimal outputs.} 
\label{con:fig:app} 
\end{figure}

 
 
In Chapter~\ref{ch:draw-and-tell}, vision and speech is integrated to recognize object names. In this work, we only fuse the class probability from the speech recognition engine and the webly-supervised object recognizer. A deep integration of the two streams of information probably will help for better accuracy, especially given the great success of multi-stream convolutional neural networks in integrating different sources of information, such as RGB color and optical flow~\citep{two:stream:cnn}, image and audio~\citep{Noda2014}, and image and language~\citep{show:tell:caption}.  Although the method already yields good recognition accuracy, the performance drops as the number of object increases. This implies that a better (deeper) integration of the two sources of information is necessary if the number of object classes of interest is very large for the annotation.  


\textbf{Other forms of annotation for the place of objects}. The location of object is indicated by scribbles in Chapter~\ref{ch:draw-and-tell}. While scribbles are very natural to draw and  are suitable for stuff classes such as sky and road, other forms of annotations might be more informative for well-shaped objects such as cars and pedestrians. Examples include coarse bounding boxes and ellipses. Bounding-box is especially worth considering, given the fact that numerous learning approaches (\eg object detectors) have been developed for bounding-box based annotations.  Furthermore, investigating when to use what forms of the annotation is interesting, and a solution to adaptively select the most suitable form will boost the annotation efficiency even further.  


\textbf{Combing with other cost-effective annotation methods}. As stated in Related Work~\ref{ch:relatedwork}, there exist many techniques for cost-effective annotations, most of which are perpendicular to our method, such as active learning, human-in-the-loop, crowd-sourcing, and gaming.  An obvious extension is to suggest the most informative image to annotate with Draw\&Tell.
Our method is also `naturally'  suitable for mobile devices, in which the microphone is already integrated and the hand-input devices are not easily accessible as for work stations.  Annotating a large dataset is always a practical contribution to the community. 
Crowd-sourcing the method or making the method available as an app for mobile devices seems to be a good step towards constructing a larger dataset in terms of the number of images or the number of object classes.


\textbf{Integrating ensemble projection into deep neural network}. The  representation learning by ensemble projection (Chapter~\ref{ch:ensemble}) is performed on an ensemble of training set with sampled surrogate class labels. This scenario shares high similarity with that of the feature learning work developed in~\citep{cnnfet14}. Inspired by this work, we see the possibility of formulating our ensemble projection as an `unsupervised' neural network training problem. A straightforward solution  is to add the non-linearity transformation of ensemble projection to the top of the standard neural network~\citep{deep:bmvc14} and use the classification scores of the surrogate classes to fine-tune the network.  The integration circumvents the two-step procedure in the training and testing of ensemble projection.   


\textbf{Distilling knowledge of neural networks via metric imitation}. The cheap features used in metric imitation (Chapter~\ref{ch:mi}) are all shallow features, such as Gist and LBP, and the transformation learned is a linear transformation. Extending it to network imitation is a great merit to have: to fine-tune (corresponding to a non-linear transformation) a shallower network by imitating the distance metric computed by a deep neural network. 
With the advance of deep learning, there are numerous of deep neural networks developed~\citep{deepnet:nips12, vgg16, googlenet, ResNet}. The networks are of very different complexity: some are deeper than others, some are wider than others. Often, the top-performing ones  are the deepest ones. 
Knowledge distillation~\citep{DistillingCNN} is a technique developed train lighter (student) networks by imitating deeper (teacher) networks by producing similar class scores for a large pool of images.  Investigating the potential of using metric imitation for knowledge distillation is intriguing, which means the distilled knowledge is in the form of distance metrics. The advantage of doing so is that this can be done for a new task in a fully unsupervised manner, \ie no annotated labels are needed.  This constitutes our future work. 


\textbf{Performance prediction and evaluation for more tasks}. As shown by our work in Chapter~\ref{ch:forecasting} and Chapter~\ref{ch:scale-aware} and the previous works~\citep{matchability, flow:confidence, completion:quality}, performance prediction makes vision algorithms more intelligent and efficient, be it for dynamic resource allocation over samples, be it for result re-organization or early planning to facilitate down-streamed applications. However, learning to predict algorithm's performance or algorithm's other properties has yet to explore for many other computer vision tasks. Since successes of performance prediction have been obtained in multiple tasks, there is an avenue to apply it to many more tasks. 

Computer vision is a field which advances so fast  that our `common' conceptions need to be re-evaluated and questioned regularly.  For instance, vision tasks were attacked mostly separately, and their integration has not gained so much attention until recent years.  This is because the advance in these sub-fields makes the synergy possible in practice.  This is also true for the integration of vision and language. Thus, the usefulness of task X for task Y needs to be evaluated regularly to detect the possibility of synergy as early as possible. 
