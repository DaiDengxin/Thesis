\chapter{Conclusion} 
\label{ch:conclusion}

This thesis presented methods towards cost-effective and performance-aware vision algorithms.  The primary focus was on  effective algorithms in terms of annotation cost, for which we presented two different approaches, one for efficient visual annotation and one for learning with unlabeled data.  In addition, we examined how to reduce the computational cost associated with the training and testing of vision algorithms in the context of learning with privileged information and metric learning.   Finally, we investigated how  performance-ware algorithms can be learned, and then be used to facilitate down-streamed applications. A more detailed look at the specific contributions are summarized below. 

\section{Contributions} 

In Chapter~\ref{ch:draw-and-tell}, \emph{Efficient Visual Annotation with Speech Recognition}, we proposed an efficient annotation method for semantic image segmentation by leveraging the power of speech recognition. In this method, we allow annotations to speak  objects's names while they draw strokes on them.  Object names are recognized by a combination of speech recognition and webly-supervised object recognition.  The drawn strokes are then converted to semantic heatmaps for the corresponding classes by ensemble interactive segmentation. Finally, an extension to the standard fully convolutional networks~\citep{Long_2015_CVPR} is made to accommodate the `weak' annotation. The method yields comparable results to the same CNNs architecture trained with standard  annotations of full segmentation masks, while being 10x faster. 
 
In Chapter~\ref{ch:ensemble}, \emph{Representation Learning with Unlabeled Data}, we developed a method to learn new feature representations by exploring the data distribution patterns of unlabeled data.  The leaning takes advantages of discriminative learning and ensemble learning to effectively exploit  manifold-smoothness assumption: surrogate classes are sampled from the manifold of data samples, on which discriminative learning is performed to extract the high-level knowledge; the noise of the sampled training training set is mitigated by ensemble learning. The learned discriminative classifiers are used to generate the new high-level feature representations. Experiments on eight datasets show that the learned representations are superior to the standard feature representations. 

In Chapter~\ref{ch:svmplus}, \emph{Fast Training Algorithms for SVM+}, we developed  efficient training algorithms to the standard approach SVM+.  New fomulations with fewer constraints are formulated in the dual domain, making it solvable efficiently by the SMO algorithm of one-class SVM.  Experiments show that our proposed algorithms achieve significant speed-ups to  the state-of-the-art solvers for  SVM+.
  
In Chapter~\ref{ch:mi}, \emph{Efficient Metric Computation via Imitation}, we developed a method to allow computationally cheap features to imitate better-performing but computationally more expensive features. We treat the problem as a transfer learning, where the neighborhood property expensive features are quantified into manifold structures. The manifold structures are view-independent, and can then be transferred to the space of cheap features. Finally, a linear transformation of the cheap features is learned so that the manifold can be approximated as well as possible.  The leaned transformation significantly boosts the performance of cheap features while retaining their efficiency.  Experiments on multiple experiments validate the efficacy of the method. 
  
In Chapter~\ref{ch:forecasting}, \emph{Performance Prediction: Success or Fail?}, We predicted how likely texture synthesis succeed on particular texture samples. To the aim, we collected a dataset of $21, 302$ textures and performed $4$ standard synthesis methods on it. The synthesized results were annotated based on their visual quality in terms of three levels. A set of relevant features were defined in order to learn a regressor to predict the performance of the methods. Extensive experiments show that the performance of texture synthesis methods can be predicted accurately.  

In Chapter~\ref{ch:scale-aware}, \emph{Performance Prediction: Under-, Properly-, or Over-Processed?}, we presented a method to predict whether a segment from image segmentation methods is under-segmented, properly-segmented, or over-segmented.  The prediction is then used to re-align the tree structure of hierarchical image segmentation, so that the depth of a regions is better coupled with its scale.  The improved hierarchy improves the quality of the hierarchical segmentation representations.


In Chapter~\ref{ch:SR4VT}, \emph{Performance Evaluation: Useful for Other Vision Tasks?}, we extensively evaluated the usefulness of image super-resolution for four standard vision tasks. This work has formalized the conception that image super-resolution is helpful for other vision tasks when the input images are of low-resolution.  


\section{Perspectives} 

\textbf{Deeper integration of vision and speech}. In Chapter~\ref{ch:draw-and-tell}, vision and speech is integrated to recognize the name of the object. In this work, we only fuse the class probability from the speech recognition engine and the webly-supervised object recognizer. A deep integration of the two streams of information probably will help for better accuracy, especially given the great success of multi-stream convolutional neural networks in integrating different sources of information, such as RGB color and optical flow~\citep{two:stream:cnn}.  Although the current method yields good recognition accuracy, the accuracy drops as the number of object increases. This implies that a better (deeper) integration of the two sources of information is necessary if the number of object classes of interest is very large.  

In a broader context, a better integration of vision and speech can lead to voice-user interface to pictures.  
Voice-user interface (VUI) has become more commonplace, and people are increasingly taking advantage of its good values ? it is hands-free, eyes-free, and far more mobile. As we people in 21th century communicate in pictures,  there is a strong need to have a VUI to talk to pictures in order to perform scene-relevant tasks, such as image tagging, story logging, and visual question answering. Also, a VUI to pictures can offer assistance to blind people, and dictate robotics to react to real visual scenes (situations) in a hand-free manner such as in clinical robotic surgery. We hope this work can inspire more research effort in integrating vision and speech. 


\textbf{Other forms of annotation for the place of objects}. The location of object is indicated by scribbles in Chapter~\ref{ch:draw-and-tell}. While scribbles are very natural to draw and  they are very suitable for stuff classes such as sky and road, other forms of annotation might be more informative for well-shaped objects such as cars and pedestrians. Examples of other annotation forms include coarse bounding boxes and ellipses. Bounding-box is especially helpful, given  the fact that numerous learning approaches have been developed for bounding-box based annotations.  Furthermore, investigating when to use what forms of the annotation is interesting, and a solution to it will boost the annotation efficiency even further.   


\textbf{Combing with other cost-effective annotation methods}. As stated in Related Work~\ref{ch:related}, there exist many techniques for cost-effective annotations, most of which are perpendicular to our method, such as active learning, human-in-the-loop, crowd-sourcing, and gaming.  Also, our method is `naturally'  suitable for mobile devices, in which the microphone is already integrated and the hand-input devices are not as easily accessible as that for work stations. Annotating a large dataset is always a practically valuable contribution to the community. Crowd-sourcing the method or making the method available as an app for mobile devices seems to be a good step towards constructing a large dataset with a large number of objectsj.  
 

\textbf{Integrating ensemble projection into deep neural network}. The  representation learning by ensemble projection (Chapter~\ref{ch:ensemble}) is performed on an ensemble of training set with sampled surrogate class labels. This scenario shares high similarity with that of the feature learning work developed in~\citep{cnnfet14}. Inspired by \citep{cnnfet14}, we find the possibility of formulating our ensemble projection as an `unsupervised' neural network training problem. A straightforward solution  is to add the non-linearity transformation of ensemble projection to the top of the standard neural network~\citep{deep:bmvc14} and use the classification scores of the surrogate classes to fine-tune the network.  

 
\textbf{Distilling knowledge of neural networks via metric imitation}. The cheap features used in our Metric Imitation (Chapter~\ref{ch:mi}) are all shallow features, such as Gist and LBP, and the transformation learned is a linear transformation. Extending it to network imitation is a great merit to have: to fine-tune (corresponding to a non-linear transformation) a shallower network by imitating the distance metric computed by a deep neural network. 
With the advance of deep learning, there are numerous of deep neural networks developed~\citep{deepnet:nips12, vgg16, googlenet, ResNet}. The networks are of very different complexity: some are deeper than others, some are wider than others. Often, the top-performing ones  are the deepest ones. 
Knowledge distillation~\citep{DistillingCNN} is a technique developed train lighter (shallower) networks by imitating deeper networks -- to produce similar class scores for a large pool of images.  Investigating the potential of using metric imitation for knowledge distillation is intriguing, and constitutes  our future work. 


\textbf{Performance prediction for more tasks}. xxx 
 


