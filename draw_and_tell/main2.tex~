\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{url}
\usepackage{sidecap}
\usepackage{rotating}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission
\newcommand*{\imouse}{\includegraphics[scale=0.03]{fig1/mouse1.png}}
\newcommand*{\iphone}{\includegraphics[scale=0.06]{fig1/microphone.png}}
\newcommand*{\tv}[1]{ \begin{turn}{90} \text{#1} \end{turn}}

\def\cvprPaperID{326} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Draw\&Tell: Speech Annotation for Semantic Image Segmentation}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}

  Semantic image labeling has leaped forward recently due to the use
  of CNNs. However, pixel-wise annotations are required, which are very
  expensive to acquire. In this paper, we design a very natural and
  cheap annotation method, and adapt the CNNs models to accommodate
  the resultant annotations. In particular, the annotation tool draws
  on the success of speech recognition (SR) and interactive
  image segmentation (IIS). We let annotators draw scribbles on
  objects of interest while they say their class names (attributes).
  The speech is recognized automatically by a SR engine that has been
  specifically trained for the purpose. Letting annotators
  draw and tell together unleashes the expressive ability of
  human. The annotations are then converted to semantic heatmaps of
  the corresponding classes by a novel combination of IIS and ensemble
  learning. Finally, we adapt the standard CNN models to accommodate 
  the soft semantic heatmaps as training data. We show in experiments 
  (i) that our annotation requires less effort than conventional
  image-level annotation and is 13 times faster than pixel-wise
  annotation; and (ii) that our CNNs trained with our annotations yield
  results comparable to those of CNNs trained with pixel-wise
  annotations.  We believe that letting annotators speak is a natural
  and effective way for many other annotation tasks in vision, esp. those
  relevant to semantics.
 
 

  % In this paper, we investigate the problem of obtaining
  % high-quality training data at a moderate cost for semantic image
  % segmentation. To this end, we present an annotation tool by
  % drawing on the success of speech recognition (SR) and interactive
  % image segmentation (IIS). The tool is more natural (\ie cheaper)
  % than per-pixel annotation and the resultant annotations are
  % significantly more informative than the image-level
  % annotations. Similar to popular IIS methods, we let annotators
  % draw scribbles over an object in the image, and speak out its class
  % name (also attributes if necessary) in the meanwhile. A speech
  % recognition engine running behind tells the \emph{what} of the
  % object and the scribbles indicate the \emph{where}. To learn from
  % the annotations, we propose a learning method with two novelties:
  % (i) a method to convert the the annotation to semantic heatmaps of
  % object classes by combining IIS techniques and ensemble learning;
  % and (ii) an adaptation to a state-of-the-art CNN model to
  % accommodate the generated semantic heatmaps. We show in
  % experiments (i) that our annotation requires comparable (even
  % smaller) amount of effort than conventional image-level
  % annotation, but yield significantly better results; and (ii) that
  % our annotation is 10x times cheaper than per-pixel annotation but
  % yields comparable results.  We believe that the policy of letting
  % the annotator speaks is a natural way for creating annotations for
  % computer vision tasks in general, especially those relevant to
  % semantics, and test it also on image attribute annotation. The
  % results are very promising.
%

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Tremendous progress has been made during the last few years in 
semantic image segmentation due to (1) the success of
training deep convolutional neural networks~\cite{deepnet:nips12,
  Decaf:icml2014, vgg16} on large classification datasets and (2) the
flexibility of transferring CNN models pre-trained for recognition to the
task of semantic segmentation where not as much training data is
available~\cite{rcnn, Long_2015_CVPR, rcnn_crf}. Model transfer,
often called fine-tuning, partially lifts the strong need for large
training sets of CNNs. Yet, the fact remains that CNNs \emph{intrinsically} 
call for large-scale training data to unleash their learning capacity. 
As such, we can expect that the limit of CNNs for semantic image segmentation 
is far from reached and that one of the main obstacles 
is a lack of large training datasets.
%  This follows the fact that pixel-wise segmentation masks are 
% enormously expensive to create. 

\begin{figure} 
$\begin{tabular}{cccc}
\hspace{-2mm}
\includegraphics[width=0.33\linewidth]{fig1/007109.jpg} &
\hspace{-4.5mm} 
\includegraphics[width=0.33\linewidth]{fig1/007109.png} &
\hspace{-4.5mm} 
\includegraphics[width=0.33\linewidth]{fig1/007109-bbx.pdf}\\
\hspace{-2mm}  
\footnotesize{\text{(a) image }} & 
\hspace{-4mm} 
\footnotesize{\text{(b) masks [\imouse]}} & 
\hspace{-4mm} 
\footnotesize{\text{(c) b-boxes [\imouse]}} \\

\hspace{-2mm}
\includegraphics[width=0.33\linewidth]{fig1/007109-scribble1.jpg}&
\hspace{-4.5mm} 
\includegraphics[width=0.33\linewidth]{fig1/007109-point.jpg}&
\hspace{-4.5mm} 
\includegraphics[width=0.33\linewidth]{fig1/007109-keywords.pdf}\\
\hspace{-2mm} \footnotesize{\text{ \textbf{(d) scribbles} [\imouse+\iphone]}} & 
\hspace{-4mm} \footnotesize{\text{(e) points [\imouse]}} & 
\hspace{-4mm} \footnotesize{\text{ (f) keywords [\imouse]}} \\
\end{tabular}$
\caption{A comparison of our annotation (d) to other forms of
  annotations for semantic segmentation, ranging from (b) full
  segmentation masks, to (c) bounding boxes, to (e) single points, and
  to (f) image-level keywords. The devices to obtain these
  annotations are shown as well: others only use the mouse (+keyboard),
  while ours also uses voice input through a microphone.  }
\label{fig:1}
\end{figure}


\begin{figure*}[!tb]
\centering
\includegraphics[width=0.95\linewidth]{fig2/figure2-crop.pdf}
\caption{The framework of the standard fully convolutional network
  (FCN)~\cite{Long_2015_CVPR} and our heatmap-based FCN.}
\label{fig:2}
\end{figure*}

Obtaining training data for semantic segmentation consists of two main
jobs: \emph{what} and \emph{where}. \emph{What} is to annotate what
objects are present and \emph{where} is to indicate their locations in
the image.  As to \emph{what}, annotators usually need to choose class
names from some form of menus with the mouse and/or
keyboard~\cite{open:surface}. The procedure can be costly especially
when the number of classes considered is large. Imagine the amount of
work for annotators to find the class name out of a list of hundreds
of names for every object they annotate, not even to mention
attributes, views, etc. In other systems~\cite{whatpoint, coco:eccv},
binary questions ask whether object classes are present, which is also
very expensive when the number of classes is large, though exploiting
the hierarchy of classes may reduce the
cost~\cite{scalable:annotation, coco:eccv}. As to \emph{where},
various forms of annotations have been used for training: from the
full segmentation masks~\cite{rcnn, rcnn_crf, crfasrnn}, over coarse
bounding boxes~\cite{weak:seg:xu, BoxSup, ConsCNN} or just single
points~\cite{whatpoint}, to nothing spatial at all (image-level
annotation)~\cite{markov:topic, cnn:em}.  As can be seen, the more
specific the annotation, the more informative it is, but also the more
expensive to obtain.

In this paper, we present a novel annotation method, which strikes a
balance between annotation cost and informativeness provided. In
particular, annotators are asked to draw scribbles (strokes) on objects of
interest, while saying aloud their class names (attribute if necessary). The 
scribbles are recorded to solve the \emph{where}
problem, and the speech is recognized by a specially trained
recognition engine to solve the \emph{what} problem.  An example of
the annotation by our method is shown in Fig.~\ref{fig:1}, along with
that of other methods.

The method is inspired by two observations: (i) \emph{what} and
\emph{where} are handled separately in previous methods. For instance,
in~\cite{open:surface} annotators first mask out a region and then
assign a label to it; and in~\cite{coco:eccv} annotators
label the presence of objects in the first round, followed by positioning
them in the second round. This separation is unnecessary and
introduces extra cost, because the two problems are interdependent and
solved together by the annotatorsâ€™ vision. It seems the separation is due
to the fact that only a single mode of input devices was used in the
previous methods, which is the mouse (+keyboard). We lift this
restriction by including the speech channel, which allows annotators
to communicate with the computer more naturally and efficiently. 
(ii) Drawing scribbles is another natural and efficient ability, and
has been widely used for interactive image segmentation. But to the best 
of our knowledge, it has not been applied to create training data for 
semantic image segmentation. We demonstrate that combining drawing and 
speaking can tackle both the \emph{what} and \emph{where} problems, 
leading to an efficient annotation method for semantic image segmentation.
We call the method \textbf{Draw\&Tell}.  


% annotate only one class each time, and the process loops over all
% classes of interest. This is inefficient because the because one image
% usually only contains very few (say 0~5) classes.  \textbf{check coco
%   how they annotate images}

% For each category found, the individual instances
% were labeled, verified, and finally segmented. Given the inherent ambiguity of
% labeling, each of these stages has numerous tradeoffs that we explored in detail.

% unprecedented number of images 
% for category detection, instance spotting and instance segmentation

%Having obtained the annotations, we convert them to heatmaps of the
%corresponding classes by combining interactive image segmentation
%(IIS)~\cite{geodesic:star} and ensemble clustering~\cite{dai:ensemble:eccv12}. 
%In particular, each scribble (i.e. stroke) on an object is taken 
%as the foreground object scribble in IIS, and the scribbles on other 
%objects belonging to the background. Scribbles for the background 
%are augmented by another set of scribbles sampled from the complete 
%background to avoid heavy bias to annotated objects. 
%The augmentation is done by randomly sampling scribbles
%from areas which are far apart from the foreground scribbles. 
%and are highly unlikely to be objects (reflected by objectness measures).
%Applying an IIS method using the scribbles generates a mask for the 
%object considered. To further increase robustness, we follow the idea of  ensemble clustering~\cite{dai:ensemble:eccv12} to run the IIS method multiple times with different
%augmented scribbles as constraints. The resultant object masks are then averaged to get the final heatmap of the object considered. The process is repeated for all object classes in the image, and the heatmaps  for classes not present are set to zero.

Having obtained the annotations, we convert them to soft confidence
maps of the corresponding classes by an extension of interactive image
segmentation. We call these soft confidence maps semantic heatmaps.
Finally, we adapt a standard CNN model~\cite{Long_2015_CVPR} to
accommodate the semantic heatmaps as training data rather than the
standard crisp labels.  The pipeline of the method is shown in
Fig.\ref{fig:2} (right hand side), which also shows the standard fully
convolutional network~\cite{Long_2015_CVPR} for ease of comparison. We
show in experiments (i) that our annotation requires less effort than
conventional image-level annotation and are $13$ times faster than
pixel-wise annotation; and (ii) that our adapted CNN trained with the
annotations yields significantly better results than the CNN trained
with image-level training data, and yields results comparable with
those of the CNN trained with pixel-wise annotations.
 
Our contributions are mainly: (i) a new method to efficiently create
training data for semantic image segmentation by letting annotators speak,
and (ii) a method to train CNNs with scribble-based training data. 
Introducing speech recognition into visual annotation is novel and 
can be used for other vision tasks as well. 

The paper is organized as follows. Sec.\ref{sec:related} presents
related work. Sec.\ref{sec:annotation} details the
annotation method, followed by Sec.\ref{sec:method} which is devoted
to the adapted CNNs. Sec.\ref{sec:experiments} then reports on the
experiments and Sec.\ref{sec:con} concludes the paper.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:related}

Previous work related to ours mainly falls into two groups:
semantic image segmentation and integration of
vision and language (speech).

\subsection{Semantic Image Segmentation}
\textbf{Methods:} There is a rich literature of semantic image
segmentation (SIS)~\cite{texton:boost, fully-crf}, and the field has
made tremendous progress since CNNs were applied. The seminal
R-CNN~\cite{rcnn} and the follow-up systems~\cite{Long_2015_CVPR,
  rcnn_crf, crfasrnn} yield significantly better performance than
previous methods. And, as said, SIS through CNNs has probably not come
to full fruition yet due to the small size of the existing training
sets. Several successful attempts have been made to use weaker
supervision in order to reduce the annotation cost.  For instance,
\cite{cnn:mil} exploits multiple instance learning to train
FCN~\cite{Long_2015_CVPR} with image-level annotations. \cite{cnn:em,
  BoxSup, ConsCNN} leverage the power of object proposals to train FCN
with object bounding-boxes. \cite{whatpoint} presents a system to
train FCN with point annotation -- objects are indicated by single
points.  In the same vein our work tries to train FCN with annotations
that are efficient to obtain. 

%various forms of weak supervision ~\cite{weak:seg:xu}

\textbf{Supervision:} Datasets play a critical role in computer
vision. They qualitatively `define' the learning tasks and guide
research directions.
% , which has been proved in many fields, such as
% optical flow~\cite{flow:dataset} and image recognition~\cite{imagenet,
%   pascal:2011}. 
Training annotations for SIS often come as full segmentation masks
(c.f. Fig.\ref{fig:1}). The most popular ones all fall into this
category such as CamVid~\cite{camvid:data} for outdoor scenes,
NYU~\cite{NYU} for indoor scenes, PASCAL~\cite{pascal:2011} and
COCO~\cite{coco:eccv} for general objects, and
PASCAL-Context~\cite{pascal:context} for objects in context.  Creating
datasets for SIS is very expensive even with excellent annotation
tools~\cite{open:surface, label:me}. As a result, methods were
proposed to reduce the cost.  For instance, \cite{scalable:annotation,
  coco:eccv} exploit the hierarchical structures of object classes to
reduce annotation space. \cite{AFrameSel, expected:loss} exploit
active learning techniques to suggest the most informative samples to
annotate under a budget. \cite{detect:eyetr} employs eye tracking
systems to help create training data for object detection.  Our
proposal is to exploit speech recognition to help dataset creation for
SIS.

There are work also to transfer knowledge learned from datasets of
relevant tasks to the task of which the annotations are scarce.  For
example, \cite{lsda} transfers the the CNN models learned for
classification to object detection, and \cite{SuTransfer} transfers
the supervision of annotated RGB images to other data modalities such
as depth and flow. Crowd-sourcing has been widely used in the field as
well and it is complementary to all the techniques mentioned above.


% \cite{human:in:loop} provides a hybrid
% human-computer algorithm for object recognition by putting a human in
% the loop to help computers.  

% segmentation masks ~\cite{open:surface, label:me, camvid:data}
% bounding box \cite{imagenet, pascal:2011}
% point \cite{whatpoint}
% semantic object selection~\cite{object:selection}

% \textbf{crornd sourcing}
% Beat the MTurkers~\cite{beat:AMT}
% other source of input: 

% The community has also created datasets containing object attributes
% [8], scene attributes [9], keypoints [10], and 3D scene information
% [11]. This leads us to the obvious question: what datasets will best
% continue our advance towards our ultimate goal of scene understanding?

\subsection{Integration of Vision and Language (Speech)}
Research interest in integrating vision and language has increased
recently, e.g. for image/video caption generation~\cite{dataset:movie,
  caption:back, mind:eye, show:tell:caption, youtube2text,
  fgm:coling14, deep:alignment:vl, babytalk}.  The objective is to
learn from a corpus of sentences and images / video snippets to
generate meaningful descriptions for new images. One of the main
research topics is the alignment of representations from the two
different domains: vision and language. In terms of language\&vision
understanding, our goal is more conservative, because the words we
handle are restrictive and vision and language are aligned with human
help.  However, our purpose of the integration is different.
% LUC: THIS POINT IS NOT SO CONVINCING. PROBABLY AN ERROR LEFT OR RIGHT
% WOULD BE OF LIMITED INFLUENCE IN OUR CASE, AS THERE WILL BE MANY MORE
% EXAMPLES OF A CLASS,
% BUT IF ONE WANTS TO GENERATE A CAPTION FOR A SPECIFIC IMAGE, THERE IS 
% NOT MUCH ROOM FOR MISTAKESâ€¦ 

Speech-driven interfaces are going through a renaissance, with popular
commercial products like Apple's Siri. The most relevant to our work
are those integrating speech and vision. Pixeltone~\cite{pixel:tone}
and Image spirit~\cite{image:spirit} are examples that use voice
commands to guide image processing and semantic segmentation. The
difference between image spirit and our work is that they use voice
commands to refine labeling results and we use them to collect
training data. Also, image spirit uses voice commands alone, while our
method combines speech input with mouse interactions. There also is
academic work~\cite{show:tell, speech:anno:img, speech:retri:img,
  video:anno:knowledgebase} and an app~\cite{smile} that use speech to
provide image descriptions.  Again, our purpose is very different.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Speech-based Annotation}
\label{sec:annotation}

\subsection{The Draw \& Tell Annotation Tool}

This section describes our annotation method, which consists of two
parts: drawing scribbles on the objects and telling the system their 
names. Drawing scribbles is straightforward and we follow popular
interactive image segmentation (IIS) methods~\cite{geodesic:star} to
record the mouse tracks. As to the speech recognition, this is a 
complex research area in its own right, and there are still problems 
with recognizing natural speech with high accuracy~\cite{SR:review}, 
especially under uncontrolled recording. Since we use it to simplify 
the task of annotation, for which high accuracy is required, some 
constraints have to be imposed, which our annotation task naturally 
fulfills. The constraints and solutions are:
\begin{itemize}
\item Constraining the vocabulary and syntax of the utterances to
  ensure robust speech recognition. For our annotation, the vocabulary
  is restricted to the names (attributes) of the objects of interest,
  and we can also instruct annotators to follow a specific syntax.

\item Synchronizing the speech input with mouse input. We assume the
  one-to-one correspondence between scribbles and utterances. We
  instruct the annotator to only say the object names while 
  they draw the scribble on the corresponding object. The speech
  recognition engine uses this synchronization to better 
  identify speech relevant to the object.

\item Providing ways to permit correction of the recognition
  errors. The recognition result is shown in a text box close to the
  scribble right after the drawing finishes. There are two ways to
  correct the recognition errors: (i) repeat the operation (drawing +
  speaking) for the same object with the constraint that the new
  scribble overlaps with the one to be corrected (a new annotation
  otherwise); (ii) type the object name directly into the text box, and the
  system will correct the name directly. 
  
\end{itemize}

% \begin{figure*}[!tb]
% \centering
% %\includegraphics[width=0.95\linewidth]{fig2/annotation.pdf}
% \framebox(200,300){}
% \caption{annotation interface and annotation examples.}
% \label{fig:3}
% \end{figure*}

The interface of our annotation tool is shown in the supplementary
material.  For implementation, we use the state-of-the-art speech
recognition engine CMUSphinx~\cite{sphinx} and more specifically the
PocketSphinx. We re-trained a new dictionary and language model for
robust recognition, which specifically focuses on the object names of
interest. Since our text corpus is small, compared to that for
general speech recognition, the model can be trained by simply
uploading the text corpus to CMUSphinx's web
service~\cite{sphinx:webtrain}.

% , i.e. the $20$ class names of PASCAL
% VOC in our experiments. 
 
% LUC: THAT SOUNDS LIKE A MAJOR LIMITATION. 20 CLASSES IS A VERY LOW 
% NUMBER COMPARED TO WHAT CNN OBJECT DETECTION IS USED FOR THESE DAYSâ€¦
% AND WHAT DOES IT TAKE TO TRAIN FOR MORE / OTHER CLASSES. IS THAT REALLY
% EASY, ALSO FOR MANY CLASSES ? 

To increase the robustness of name
interpretation, we augment the training data for speech recognition
with synonyms (\eg table instead of desk, house instead of building) and plurals. 
% and sub-class names of the object names (\eg flower for plant, 
% puppy for dog). 
This is because annotators may have their own preferences when
choosing specific words for the same object class. The recognition
results of the augmented words are converted to their `mother' words
to obtain the final name, instead of annotators having to memorize
strictly name lists to pick from. The annotator will be asked to
review the name list if the system fails multiple times to recognize
the speech, mainly because the words are out of the training list.

% LUC: THIS WOULD BE AN ISSUE IF FINE-GRAINED CLASSIFICATION IS THE TARGET.
% PUPPET AND DOG MAY IN SUCH CASE E.G. NOT BE CONSIDERED THE SAME ANY LONGER. 

% For the acoustic model, we use the one provided
% for US English. A new model can be trained for the annotators' accent
% for better performance if necessary.
% LUC: AGAIN, THIS SOUNDS LIKE BEING QUITE COMPLICATED. IT WOULD BE GOOD
% TO ADD SOME EXPLANATION THAT THIS IS QUITE EASY TO DO ALSO FOR PEOPLE
% WHO ARE NON-EXPERTS IN SPEECHâ€¦ LIKE VISION RESEARCHERS. 

\subsection{Annotation Results} 

We annotated the 20 object classes of PASCAL VOC on $12,031$ images,
including all images in the training and validation set of the PASCAL
VOC 2012 segmentation benchmark and all images in the augmented
dataset from~\cite{semantic:contour}. To simplify the task, we also
adopt the concept of using hierarchies~\cite{scalable:annotation} by
splitting the 20 objects into 3 groups: Person+Animal, Vehicle, and
Indoor. In each round, annotators only need to check the classes in
the current group. We find that humans can handle $5\sim 8$
semantically related classes together without problems. Two annotators
annotated the data, independently, with the same goal of drawing
scribbles as much as possible on the objects, but on different
budgets: the first one has $1.5$ seconds per object, while the second
one has $3$ seconds. They will be referred as Anno-1.5 and
Anno-3. Examples of the annotations will be provided in the
supplementary material.

\textbf{Accuracy}: The annotation results  are
listed in Table~\ref{table:anno}. Four types of errors are reported:
two at object-level and two at pixel-level. At object-level, we assign
the scribbles to their closest object masks (by the number of pixels
shared) and check the consistency of their labels. False positives and
false negatives are reported, which are fairly few. Also, we must be
aware that objects of very small size may be confusing to annotators
in terms of whether they need to be annotated. These cases
contributed the most of these FP and FN cases. Scribbles that are
`correct' at object-level were evaluated for their accuracy at
pixel-level. We specify two errors: the percentage of pixels drawn
onto background as well as onto other objects. They are actually very
small, which means for the correctly detected objects humans can spot
their position very accurately.  Overall, the results of our
annotation are comparable to those of the single point annotations
in~\cite{whatpoint}, but scribbles are more informative.

\begin{table}
   \centering \small
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}  
    \hline \multicolumn{4}{|c|}{Anno-1.5} & \multicolumn{4}{c|}{Anno-3}  \\ \hline
     \multicolumn{2}{|c|}{Object} & \multicolumn{2}{c|}{Pixel} & \multicolumn{2}{|c|}{Object} & \multicolumn{2}{c|}{Pixel} \\ \hline
      FN & FP & BG  & O-Obj & FN & FP & BG  & O-Obj   \\ \hline
     2.8   & 4.3 &     1.4          & 5.6   &  2.6   & 4.4 &     0.9            & 4.2  \\  \hline
       \end{tabular}
       \caption{Errors (\%) of the annotation: false positives (FP) and false negatives (FN) at object-level, 
    and the percentages of pixels drawing to the background and to objects of other classes for the scribbles which are `correct'
at object-level. 
}
% LUC: THE DESCRIPTION OF THE PIXEL-WISE ERRORS IS DIFFERENT FROM THAT IN THE MAIN TEXT. 
% THERE IT IS SAID THAT THE \% MEAN THE AMOUNT OF PIXELS OF SCRIBBLES DRAWN OFF THE OBJECT, BUT 
% HERE THE CAPTION SEEMS TO SUGGEST IT IS THE \% OF SCRIBBLES THAT COVER PART OF THE BACKGROUND
% OR OTHER OBJECTS.
       \label{table:anno}
\end{table}


\textbf{Annotation Speed}: We compare the speed of our annotation
method to four other popular annotation methods
(c.f. Fig.\ref{fig:1}): full segmentation masks, bounding boxes,
points on objects, and image-level keywords.  Because all these
methods need to loop over all classes to solve the \emph{what}
problem, the minimum time - browsing time - is constant for
all. According to~\cite{whatpoint}, the time for browsing one image
for one class is 1 second, which is consistent with our
experiments. In order to have a fair comparison, we use the same
hierarchies for all annotation methods as ours (the three groups), and
give $2$ seconds as the browsing time for each group.  Let's assume
that the sub-classes of different groups do not co-exist in the same
image, which generally holds for the dataset.  Since there are $6.7$
classes on average for each group, the total browsing time per image
is about $3*2 + 6.7*1=12.7$ seconds, which is also the time for
image-level annotation.  The time for other forms of annotations is
the sum of this $12.7$ seconds and the time for annotating $2.8$ (on
average) objects in each image. Point-based
annotation~\cite{whatpoint} costs $3.2$ seconds for clicking points on
the objects, resulting in $15.9$ seconds in total.

For the time of drawing one bounding box, different
numbers are reported, varying from 7 to 25.5
seconds~\cite{best:two:world, annotation:strength}, probably because
the types of objects being handled and the quality of the annotation
are different. We experimented with images from PASCAL VOC 2012
and drew bounding boxes for $200$ randomly chosen objects. 
% LUC: IT WOULD
% BE GOOD TO DESCRIBE THIS EXPERIMENT A BIT MOREâ€¦ WHO WERE THE ANNOTATORS,
% HOW MANY, MALE-FEMAL, AGE, CORRECTED FOR NORMAL VISIONâ€¦ THE USUAL PSYCHO-
% PHYSICS STUFF. 
The annotation time per bounding box is $9.1$ seconds on average,
which is quite efficient compared to the numbers reported in the
literature. Thus the total time for bounding-box based annotation is
38.2 (12.7+2.8*9.1) seconds. Similarly, we annotated the masks for
$200$ randomly sampled objects using the annotation tool developed
in~\cite{open:surface}.
% LUC: SAME HEREâ€¦ MAYBE BETTER NOT DESCRIBE IN DETAIL WHEN THE ONE AND 
% ONLY ANNOTATOR WAS CALLED DENGXIN. 
Annotating each mask takes $44.3$ seconds on
average. Thus, full-mask based annotation time is $136.7$
(12.7+2.8*44.3) seconds.  For our annotations, the browsing time is the same. 
% LUC: THIS DOES NOT SOUND LIKE A FAIR COMPARISON THEN. HERE YOU ONLY 
% CONSIDER THE USE OF HIERARCHIES FOR OUR METHOD, NOT FOR THE OTHERSâ€¦ 
% THEIR 20 SECONDS OF BROWSING TIME COULD THEN ALSO BE BROUGHT DOWN TO
% JUST 6 AND ALL OVERALL ANNOTATION TIMES WOULD ALSO GO DOWN. 
Drawing one scribble takes $1.5$ and $3.0$ seconds for Anno-1.5 and
Anno-3. The recognition error rate of speech recognition is about
$10\%$ for our task, which contributes to the correction time. Putting
all together, the annotation time is $10.7$ ($3*2+2.8*1.5*(1+0.1+0.01
...)$) seconds per image for Anno-1.5, and $15.2$ seconds for
Anno-3.  All the
numbers are listed in Table~\ref{table:speed}. According to the
numbers, our annotation is the fastest one (faster than the
image-level annotation as well), due to the help by speech
recognition. Some of these numbers are quite `rough' estimations, but
they reflect the cost to a reasonable level of accuracy.

\textbf{Insights}: Other than solving the \emph{what} problem of
object annotation, speech recognition can also be used to collect
context information and image attributes, such as the view and size of
objects. \cite{ConsCNN} has proven recently that the size of objects
is a very useful constraint for semantic segmentation, and many work
including image spirit~\cite{image:spirit} has shown that the context
of objects are very helpful for object recognition. Also, an
annotation tool with speech recognition such as Draw\&Tell is
especially efficient to collect training data for open-vocabulary
learning approaches~\cite{open:voca:retr, child:learning}, where
annotators have full freedom to annotate (draw and tell) what they see
in the image. This is left to future work.


\begin{table}
   \centering \small
\setlength\tabcolsep{0.22em} {
  \begin{tabular}{|c|c|c|c|c||c|c|}  
    \hline
      &  Masks & B-Boxes & Points  & Keywords & Anno-1.5 & Anno-3  \\ \hline
    img   & 136.7  & 38.2 &  15.9 & 12.7  &  10.7     & 15.2\\  \hline
    obj  & 44.3   & 9.1  &  1.2  & 0.0   & 1.5       & 3.0 \\ \hline 
       \end{tabular} }
       \caption{The annotation speed (seconds) of different methods for one image (\emph{where} + \emph{what}) and one object (\emph{where}), measured on PASCAL VOC 2012.}
       \label{table:speed}
\end{table}


% \begin{itemize}
% \item Draw scribbles on object of target classes, and speak up its class name when you draw.
% \item check whether the recognized class name is correct; correct it if wrong.
% \item annotate next object until all objects of target classes are annoated.   
% \end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Approach}
\label{sec:method}

\begin{figure*} 
$\begin{tabular}{cccccc}
\hspace{-1.5mm}
\includegraphics[width=0.19\linewidth]{fig3/006488-scribbles.jpg} &
\hspace{-4mm} 
\includegraphics[width=0.19\linewidth]{fig3/heatmap-dog-0.jpg} &
\hspace{-4mm} 
\includegraphics[width=0.19\linewidth]{fig3/006488-scribbles-aug.jpg}&
\hspace{-4mm} 
\includegraphics[width=0.19\linewidth]{fig3/heatmap-dog-1.jpg} &
\hspace{-4mm} 
\includegraphics[width=0.19\linewidth]{fig3/heatmap-dog-all.jpg} \\
\hspace{-1.5mm}  
\footnotesize{\text{(a) original scribbles}} & 
\hspace{-4mm} 
\footnotesize{\text{(b) heatmap of (a) }} & 
\hspace{-4mm} 
\footnotesize{\text{(c) augmented scribbles}} &
\hspace{-4mm} 
\footnotesize{\text{(d) heatmap of (c)}} &
\hspace{-4mm} 
\footnotesize{\text{(e) final heatmap}} \\

\end{tabular}$
\caption{An example of scribble augmentation with corresponding
  heatmaps: (a) annotated scribbles; (b) the heatmap of the dog with
  the scribbles in (a); (c) augmented scribbles for the background;
  (d) the heatmap of the dog with the scribbles in (c); (e) the final
  heatmap of the dog by averaging $T$ heatmaps (such as (d)) with
  different augmented scribbles.}
\label{fig:3}
\end{figure*}

We follow recent methods~\cite{rcnn, Long_2015_CVPR, crfasrnn} to
fine-tune CNNs for semantic image segmentation. To this end, we
convert the annotated semantic scribbles to semantic heatmaps --
confidence maps of corresponding classes -- and then adapt the fully
connected CNNs (FCN)~\cite{Long_2015_CVPR} to accommodate the
heatmap-based training data.

\subsection{Scribbles to Heatmaps} 
\label{sec:scribble}

% LUC: THE PAPER IS QUITE REPETITIVE FOR SOME ASPECTS, LIKE THE
% EXPLANATION OF THE HEATMAPS, WHICH WAS IN CRUDE TERMS ALREADY
% DISCUSSED BEFOREâ€¦ MAYBE BETTER TO EARLIER ONLY SAY WHAT MATTERS
% THEN. THE SAME GOES FOR A COUPLE OF OTHER PARTS.

We convert the annotated scribbles to semantic heatmaps of all classes
considered.  Let $I \in \mathbb{R}^{W \times H \times Z}$ be the input
image, with size $[W, H]$ and depth $Z$ (3 for RGB images), its
semantic heatmaps are denoted by $P^k \in [0,1]^{W \times H}$, where
$k \in \{1, 2, ...,K \}$, and $K$ is the number of classes
considered. For classes not present in the images, their heatmaps are
simply set to zero.  For classes which are present, we generate the
heatmap for each class individually by using interactive image
segmentation (IIS). In particular, we take the scribbles of a class as
the scribbles for the foreground object in the context of IIS, and the
scribbles on other objects as that for the
background. Fig.~\ref{fig:3}(a) shows an example, where the scribble
on the dog is considered as the foreground one, and the one on the
person is taken as scribble for the background. As the example shows,
only having the one scribble for the entire background is wanting (see
Fig.\ref{fig:3}(b)).  We want to add scribbles sampled from the rest
of the background as well, thus forming an augmented scribble set. See
Fig.~\ref{fig:3}(c) for the three additional scribbles. With all
scribbles in the augmented set, we run the IIS method proposed
in~\cite{geodesic:star} to get one solution to the heapmap $P^k$
(Fig.\ref{fig:3}(d)), with $1$ indicating foreground and $0$
background.
%LUC: IT WOULD BE GOOD TO SEE IN THE FIGURE ALSO THE DOG HEATMAP WITH 
%ONLY THE MAN SCRIBBLE FOR BACKGROUND VS. FOR THE AUGMENTED BACKGROUND 
%SCRIBBLE SET. 
To further increase the robustness, we follow the
idea of ensemble sampling~\cite{dai:ensemble:eccv12} to run the method
$T$ times with different augmented scribbles to obtain $T$ such
solutions. 
%LUC: SHOULDNâ€™T YOU SAY HEATMAPS INSTEAD OF MASKS? STICK TO THE SAME
%NAME FOR THE SAME THING THROUGHOUT, OTHERWISE IT GETS CONFUSING FOR
%READERS. 
The final heatmap (shown in Fig.~\ref{fig:3}(e)) for class $k$ is
computed as the average of all the individual solutions:
\begin{equation}
  \label{eq:heatmap}
  P^k = 1/T \sum_{t=1}^T P^k_t.
\end{equation}
%LUC: THIS IS GETTING QUITE UNCLEAR. BEFORE THE HEATMAP WAS CALLED P,
%HERE A CONCEPT G_t APPEARS, WHICH IS UNDEFINED. ALSO, AS FAR AS THE 
%READER IS CONCERNED, A HEATMAP IS BINARY AS THAT IS WHAT HE WAS TOLDâ€¦ 
%HERE IT ALL OF A SUDDEN STOPS BEING SO. 
Next, we describe how the additional scribbles are sampled.

%LUC: FIG.3 TALKS ABOUT THE GEODESIC DISTANCE MAP, BUT THIS COMES OUT OF THE BLUE WHEN FIG.3 IS INITIALLY REFERRED TO. THAT CONCEPT NEEDS SOME  FORM OF EXPLANATION / CONTEXT. 

\subsubsection{Scribble Augmentation}
% \subsubsection{Geodesic distance map}
% We follow the notations used in~\cite{geodesic:star} for geodesic
% distance.

For each class, three (sufficient in practice) more scribbles are
added. For simplicity, they are added sequentially.  New scribbles
should be far from the foreground for good separation, and far from
existing background scribbles to be complementary.  Below, we define
the distance between the scribbles.

%Given two pixels $a$ and $b$ in the image $I$, where $a, b
%\in\{1, ..., W \times H\}$, 
%LUC: NOT SURE WHETHER THAT DEFINITION OF a AND b AS SCALARS IS 
%HELPFULâ€¦ IT ACTUALLY SEEMS WRONG FOLLOWING FURTHER DEFINITIONS. 
%I ALSO REFORMULATED SOME OF WHAT FOLLOWS. 
Given an image $I$, all paths that connect pixel $a$ and pixel $b$ are
denoted by $\mathcal{Q}_{ab}$.  Suppose we have a path $\Gamma$
described by the pixels it passes through $\{ \Gamma^1, \Gamma^2,...,
\Gamma^r\}$. The distance between the pixels $\Gamma^1$ and $\Gamma^r$
along path $\Gamma$ is defined as (following~\cite{geodesic:star}):
\begin{equation}
  \label{eq:gdist}
  D(\Gamma) = \sum_{j=1}^{r} \sqrt{ d_{eu}(\Gamma^j, \Gamma^{j+1}) + \lambda \| \triangledown I(\Gamma^j)\|^2}  \enspace ,
\end{equation}
where $d_{eu}(\Gamma^j, \Gamma^{j+1})$ is the Euclidean distance
between two consecutive pixels, and $\| \triangledown I(\Gamma^j)\|^2$
is the gradient magnitude between $(\Gamma^j,\Gamma^{j+1})$, which is
computed by the edge detector of the structured random
forest~\cite{edge:srf}, to avoid texture edges. $\lambda$ is used to
balance the two terms.

Let's denote the existing scribbles as a set of pixels
$\mathcal{E}$. Then, the geodesic distance of pixel $a$ to $\mathcal{E}$ is defined as:
\begin{equation}
  \label{eq:gdist2}
  G(a) =  \min_{b\in \mathcal{E}}  \min_{\Gamma \in \mathcal{Q}_{ab}} D(\Gamma) 
%d_g(a,k) = min
\end{equation}
Without any specific preference, we fix the shape of the new scribble
$\bar{\mathcal{E}}$ to a disk of radius $r=20$ (a balance between
localization and informativeness). Then, the geodesic distance from
the scribble centered at position $a$ to existing scribbles
$\mathcal{E}$ is:
\begin{equation}
  \label{eq:gdist:set}
  G(\bar{\mathcal{E}}_a)  =  \min_{c \in \bar{\mathcal{E}}_a} G(c).
\end{equation}


%LUC: I DONâ€™T UNDERSTAND THIS STORY ABOUT FLAT CIRCLES. IN FIGURE 3
%THE ADDITIONAL SCRIBBLES ON THE BACKGROUND CERTAINLY ARE NO CIRCLES
%AT ALL (THE BLACK ONES). AND WHAT IS THIS `ONE EXAMPLEâ€™ IN THE NEXT
%SENTENCE ABOUT? I GUESS YOU WOULD HAVE TO FIX A CIRCLE BEFORE THESE
%DISTANCES CAN BE CALCULATEDâ€¦ WHERE IS THAT CIRCLE CHOSEN IN THE IMAGE
%FOR THIS EXAMPLE? OR IS THIS THE DISTANCE A CALCULATED FOR EACH POINT
%AS CENTER a ? THEN IT IS NOT REALLY ONE EXAMPLE, BUT THE FULL SET OF
%DISTANCES, I.E. FOR ALL POSSIBLE CHOICES OF CENTER aâ€¦ 

%One example for $G( \bar{\mathcal{E}}_a)$ is shown in
%Fig.~\ref{fig:3}(b) for the existing scribbles on the dog and the
%person in Fig.~\ref{fig:3}(a).
The sampling probability for the next scribble is then defined as: 
\begin{equation}
  \label{eq:sampling}
  Pr(a) = \frac{\text{exp}(G(\bar{\mathcal{E}}_a)^2 / \sigma^2)} {\sum_{b=1}^{WH} \text{exp} (G(\bar{\mathcal{E}}_b)^2 / \sigma^2)}
\end{equation}
where $\sigma$ is set adaptively to the average of all $G(\bar{\mathcal{E}}_a)$.  
$Pr(a)$ needs to be updated each time a new
scribble is added. One example of $Pr(a)$ for all values of $a$ is shown in Fig.\ref{fig:4}. 
The randomness introduced by this sampling allows for the ensemble approach. Note that other information such
as objectness~\cite{edge:box, learning:proo} can be exploited for sampling the background
scribbles. We leave this as future work.


\begin{SCfigure} 
$\begin{tabular}{cccccc}
\hspace{-2mm}
\includegraphics[width=0.3\linewidth]{fig3/006488-scribbles.jpg} &
\hspace{-4mm} 
\includegraphics[width=0.3\linewidth]{fig3/006488-geodesic.jpg} \\
\hspace{-2mm}
\footnotesize{\text{(a) existing scribbles}} & 
\hspace{-4mm} 
\footnotesize{\text{(b) probability map}} \\ 
\end{tabular}$
\caption{
Probability map for sampling the next new scribble
  (\cf Eq.\ref{eq:sampling}).}
\label{fig:4}
\end{SCfigure}

\subsection{Heatmap-based FCN}

FCN~\cite{Long_2015_CVPR} is designed to regress the 2D label map $O
\in \{1,2,..., K\}^{W \times H}$, directly from the input image $I$.  The output of FCN is 
$K$ score maps $S$, one for one class, i.e. $S \in \mathbb{R}^{W \times 
H \times K}$.  The class labels are obtained either by simply taking the
best-scoring classes at each pixel~\cite{Long_2015_CVPR} or by using
conditional random fields for further refinement~\cite{cnn:em}. FCN 
is trained to minimize the prediction error of cross-entropy over all 
pixels, and its optimization function is:
\begin{equation}
  \label{eq:fcn}
  \mathcal{L} = \sum_{n=1}^N  \sum_{a=1}^{WH}  \mathcal{L}_{na}(S_{na}, O_{na}),
\end{equation}
where $N$ is the number of training images and $\mathcal{L}_{na}$ is
the per-pixel loss function, which is commonly defined as: 
\begin{equation}
  \label{eq:fcnloss}
\mathcal{L}_{na} =  -\text{log} (\frac{ \text{exp}(S_{na}^{O_{na}})} {\sum_{k=1}^K \text{exp}(S_{na}^k)}).   
\end{equation}
The loss function is only computed for pixels having ground truth
labels. Otherwise, the loss function is set to zero.
 

Directly applying FCN to our training annotations is problematic, as
our annotations are soft semantic heatmaps rather than crisp
segmentation masks. To solve this, we extend the loss function in
Eq.\ref{eq:fcnloss} to the following:

\begin{equation}
  \label{eq:ourloss}
\mathcal{L}_{na}^\prime =  \sum_{k=1}^K P_{na}^k \left[ -\text{log} (\frac{ \text{exp}(S_{na}^k)} {\sum_{k^{\prime}=1}^K \text{exp}(S_{na}^{k^\prime})}) \right].    
\end{equation}
By the adaptation, the loss function is modulated by the heatmaps of
relevant classes -- for each pixel, the most confident classes affect
the loss function the most.  It can be seen that the loss function on
crisp segmentation masks in Eq.\ref{eq:fcnloss} is a special case of
our new loss function. The new loss function can be optimized like 
for the normal FCN, with a modification to the loss layer.
We call the model heatmap-based FCN (HFCN), and its pipeline is shown in Fig.~\ref{fig:2}.


\section{Experiments}
\label{sec:experiments}
We evaluate HFCN with different settings, and compare it to other
competing methods. The goal is to show that training with
scribbles-based annotations is a good trade-off between annotation
cost and prediction accuracy.

\subsection{Experimental Settings}
\label{sec:settings}

\textbf{Dataset}: We evaluate HFCN on PASCAL VOC
2012~\cite{pascal:2011}, which comes with three subsets: training
($1464$ images), validation ($1449$ images) and test ($1456$ images),
having $20$ object classes annotated in full segmentation masks. In
order to keep the same settings with previous
methods~\cite{Long_2015_CVPR, ConsCNN, cnn:em, whatpoint}, we also
extend the training set by adding the extra annotations created by
Hariharan \etal~\cite{semantic:contour} (excluding images from the
original validation set), ending up with $10,582$ training images. The
method is evaluated on the validation set and the test set, under the
metric intersection-over-union (IoU) for all the $20$ classes.

\textbf{CNN}: As to the CNN architecture, we adopt the
FCN-8s~\cite{Long_2015_CVPR} model, as it has shown excellent
performance and there is code available. The FCN model is adapted from
the VGG network~\cite{vgg16} pre-trained on the ILSVRC
dataset~\cite{imageNet:challenge}. For the optimization, we employ a
procedure similar to FCN: the SGD solver is used, with an initial
learning rate of $10^{-6}$; the network is trained with $80,000$
iterations.  HFCN is implemented with the Caffe
framework~\cite{Caffe}.% We use a mini-batch of $10$ images.
% Fne-tuning of HFCN on the \emph{train-aug} set of PASCAL VOC 2012
% takes about $30$ hours on a NVIDIA Tesla K40 GPU.

The parameter $\lambda$ in Eq.\ref{eq:gdist} is set the same way as used in~\cite{geodesic:star}. 
$T$ of the ensemble sampling is set to $10$, which is found to be sufficient for the augmentation. 

\begin{table*}[tb]
\centering
\setlength\tabcolsep{0.28em} {
\begin{tabular}{|c|c|c|c|c||ccc|c}   \hline
Supervision  & Image-level & Point & Bounding-box & Full-mask   &     \multicolumn{3}{c|}{Scribbles (ours)}   \\  
Method & ConsCNN~\cite{ConsCNN}  & WhatPoint~\cite{whatpoint} & CNN-EM~\cite{cnn:em}  &    FCN-8s~\cite{Long_2015_CVPR}  & HFCN-1.5 & HFCN-3  & HFCN-1.5+3   \\   
mIoU & 35.3 &  42.7     & 60.6 & 62.7  & 56.2 & 61.9 & 62.4 \\  \hline
\end{tabular}}
\caption{The results of different methods with varying levels of supervision on the validation set of PASCAL VOC 2012.}
\label{table:val}
\end{table*}

  % \begin{table}[tb]
 %    \centering
 %    \begin{tabular}{|c|c|c|c|c|c|c|}
 %     Method & Supervision  & val IOU  & test IOU \\ \hline
 % % image-level & point-based & bounding-box & full-mask & ours  \\ \hline
 %      % 32.0        &  42.7   &      
 %    \end{tabular}
 %    \caption{comparison to other methods}
 %  \end{table}

  \begin{table*}[tb]
\small
    \centering
%\def\arraystretch{1.2}
\setlength\tabcolsep{0.22em} {
    \begin{tabular}{|cccccccccccccccccccccc|cccccccccc}
      \hline 
     Method & \tv{aerop}  & \tv{bike}  & \tv{bird} & \tv{boat} &\tv{bottle} &\tv{bus}  &\tv{car}  &\tv{cat}  &\tv{chair}  &\tv{cow}  &\tv{table}  &\tv{dog}  &\tv{horse}  &\tv{mbike}  &\tv{person}  &\tv{plant}  &\tv{sheep}  &\tv{sofa}  &\tv{train}  &\tv{tv}  & mIoU \\ \hline
\multicolumn{1}{|l}{\textbf{Image-level}:} &&&&&&&&&&&&&&&&&&&&& \\ 
MIL-FCN~\cite{cnn:mil} & --& --& --& --& --& --& --& --& --& --& --& --& --& --& --& --& --& --& --& --& 24.9  \\   
Img2Pix-CNN~\cite{img:pix:cnn} & 25.4& 18.2& 22.7& 21.5& 28.6& 39.5& 44.7& 46.6& 11.9& 40.4& 11.8& 45.6& 40.1& 35.5& 35.2& 20.8& 41.7& 17.0& 34.7& 30.4&32.6 \\   \hline
\multicolumn{1}{|l}{\textbf{Single-Point}:} &&&&&&&&&&&&&&&&&&&&& \\ 
What's-point~\cite{whatpoint} & -- & -- & -- & -- & -- & -- & --& --& --& --& --&-- &-- &-- & --& --& --& --& --&-- & 43.6 \\   \hline

\multicolumn{1}{|l}{\textbf{Bounding-Box}:} &&&&&&&&&&&&&&&&&&&&& \\ 
CNN-EM~\cite{cnn:em} & 64.4& 27.3& 67.6& 55.1& 64.0& 81.6& 70.5& 76.0& 24.1& 63.8& 58.2& 72.1& 59.8& 73.5& 71.4& 47.4& 76.0& 44.2& 68.9& 50.9& 60.8  \\   
BoxSup~\cite{BoxSup} & 80.3& 31.3& 82.1& 47.4& 62.6& 75.4& 75.0& 74.5& 24.5& 68.3& 56.4& 73.7& 69.4& 72.5& 75.1& 47.4& 70.8& 45.7& 71.1& 58.8& 64.6  \\   \hline
\multicolumn{1}{|l}{\textbf{Full-mask}:} &&&&&&&&&&&&&&&&&&&&& \\ 
SDS~\cite{SDS}                        & 63.3& 25.7& 63.0& 39.8& 59.2& 70.9& 61.4& 54.9& 16.8& 45.0& 48.2& 50.5& 51.0& 57.7& 63.3& 31.8& 58.7& 31.2& 55.7& 48.5& 51.6 \\   
FCN-8s~\cite{Long_2015_CVPR}          & 76.8& 34.2& 68.9& 49.4& 60.3& 75.3& 74.7& 77.6& 21.4& 62.5& 46.8& 71.8& 63.9& 76.5& 73.9& 45.2& 72.4& 37.4& 70.9& 55.1& 62.2 \\   
Zoomout~\cite{zoomout:fet}            & 81.9& 35.1& 78.2& 57.4& 56.5& 80.5& 74.0& 79.8& 22.4& 69.6& 53.7& 74.0& 76.0& 76.6& 68.8& 44.3& 70.2& 40.2& 68.9& 55.3& 64.4 \\   
DeepLab-CRF~\cite{rcnn_crf}  & 83.5& 36.6& 82.5& 62.3& 66.5& 85.4& 78.5& 83.7& 30.4& 72.9& 60.4& 78.5& 75.5& 82.1& 79.7& 58.2& 82.0& 48.8& 73.7& 63.3& \textbf{70.7} \\   \hline
\multicolumn{1}{|l}{\textbf{Speech-Scribbles}:} &&&&&&&&&&&&&&&&&&&&& \\ 
HFCN-1.5 & 72.1& 32.1& 63.2& 47.0& 59.7& 72.1& 70.4& 72.8& 21.6& 58.2& 41.6& 68.2& 58.2& 71.3& 71.5& 41.4& 56.8& 32.1& 64.5& 52.1&  56.4\\  
HFCN-3     & 76.1& 37.4& 69.3& 53.5& 64.9& 79.6& 74.4& 76.4& 25.9& 62.5& 45.8& 72.3& 62.4& 76.8& 74.6& 45.7& 72.1& 38.3& 68.9& 56.2&  61.7\\  
HFCN-1.5+3 & 76.7& 35.5& 69.7& 50.8& 62.5& 76.6& 75.8& 78.7& 23.9& 63.6& 47.9& 73.1& 63.9& 78.4& 74.6& 46.5& 72.9& 37.9& 71.9& 57.5&  62.0\\   \hline      % 32.0        &  42.7   &      
    \end{tabular}  
}
\caption{Comparison to other methods with different levels of supervision on PASCAL VOC 2012 test.}
\label{table:voc12:test} 
  \end{table*}
  
\subsection{Results}
\textbf{Validation Results}: We first evaluate the method on the
validation set.  Table~\ref{table:val} show the results, where the
results of several other methods are also reported for comparison. For
HFCN, we trained it with three versions of annotations: scribbles from
(1) Anno-1.5; (2) Anno-3; and (3) Anno-1.5 + Anno-3. They are referred
hereafter by HFCN-1.5, HFCN-3, and HFCN-1.5+3. It generally true from
the table that stronger (more expensive) supervision leads to better
performance. However, it also shows that with the scribbles by
Anno-1.5, HFCN is already able to yield quite decent results. If the
training data is upgraded to the scribbles of Anno-3, the results are
comparable to that of FCN-8s~\cite{Long_2015_CVPR} and better than
that of CNN-EM~\cite{cnn:em}, though their training annotations are
much more expensive to obtain (\cf Table~\ref{table:speed}). Our
method also shows significantly better results than the
methods~\cite{whatpoint, ConsCNN} trained with comparable annotation
cost.  HFCN-1.5+3 performs better than HFCN-3, but the improvement is
not significant. This is mainly because for the same object human tend
to draw similar scribbles under the constraints imposed by the layout
of the objects, leading to redundant annotations.


\textbf{Test Result}: We also evaluate HFCN on the test set. Table~\ref{table:voc12:test} shows
all the results. The conclusions drawn on the validation set hold on the test set as
well. Our method obtains results which are comparable to other methods
trained with more expensive supervision, \ie full masks and bounding
boxes, and are better than methods trained with supervision of
comparable annotation cost. These annotation costs are only computed
on the PASCAL VOC training images, however, some competing methods
also use `extra' supervision. For instance,
Img2Pix-CNN~\cite{img:pix:cnn} learns image prior from another larger
set of images; BoxSup~\cite{BoxSup} uses the MCG object
proposal~\cite{MCG} method, which is trained with full-mask
supervision from the PASCAL VOC dataset; and CNN-EM~\cite{cnn:em} uses
a hold-out set of full-mask images to learn the CRF model.  Also, the
bounding boxes used by the two methods~\cite{cnn:em, BoxSup} are
generated from the labeled full segmentation masks, which may transfer
some supervision from there because the generated bounding boxes are
generally tighter than those annotated directly by annotators.  In
this work, we stick to only the supervision from our annotations in
order to better show the potentials. We conjecture that the
performance of HFCN will be improved the same way as other
methods~\cite{rcnn_crf, cnn:em, ConsCNN} by further refining the
results with graphical models such as the
CRF~\cite{fully-crf}. Fig.~\ref{fig:5} shows several labeling
examples.



\begin{figure*} 
\scalebox{1}{
$\begin{tabular}{cccccc}
\hspace{-2mm}
\includegraphics[width=0.24\linewidth]{fig5/2007_001678.jpg} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2007_001678_gt.png} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2007_001678_res_1.png} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2007_001678_res_2.png} \\

\hspace{-2mm}
\includegraphics[width=0.24\linewidth]{fig5/2007_000346.jpg} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2007_000346_gt.png} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2007_000346_res_1.png} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2007_000346_res_2.png} \\

% \hspace{-2mm}
% \includegraphics[width=0.24\linewidth]{fig5/2007_006444.jpg} &
% \hspace{-4mm} 
% \includegraphics[width=0.24\linewidth]{fig5/2007_006444_gt.png} &
% \hspace{-4mm} 
% \includegraphics[width=0.24\linewidth]{fig5/2007_006444_res_1.png} &
% \hspace{-4mm} 
% \includegraphics[width=0.24\linewidth]{fig5/2007_006444_res_2.png} \\

\hspace{-2mm}
\includegraphics[width=0.24\linewidth]{fig5/2007_000925.jpg} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2007_000925_gt.png} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2007_000925_res_1.png} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2007_000925_res_2.png} \\

\hspace{-2mm}
\includegraphics[width=0.24\linewidth]{fig5/2009_005260.jpg} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2009_005260_gt.png} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2009_005260_res_1.png} &
\hspace{-4mm} 
\includegraphics[width=0.24\linewidth]{fig5/2009_005260_res_2.png} \\

\hspace{-2mm}
\footnotesize{\text{(a) input image}} & 
\hspace{-4mm} 
\footnotesize{\text{(b) Ground truth}} & 
\hspace{-4mm} 
\footnotesize{\text{(c) HFCN-1.5 }} & 
\hspace{-4mm} 
\footnotesize{\text{(d) HFCN-3}} \\ 
 \end{tabular}$}
\caption{Examples of the segmentation results on the validation set of PASCAL VOC 2012.}
\label{fig:5}
\end{figure*}


\textbf{Annotation Cost vs. Accuracy}: We tabulate the annotation cost and the
mIoU of all the methods considered. See Fig.~\ref{fig:6} for the
results. From the plot, it is evident that HFCN strikes a good balance
between annotation speed and prediction accuracy. The plot also raises
an interesting question for semantic segmentation: under the same
annotation budget, should one annotate more data of `weak' supervision
or less data of `strong' supervision, or a mixture of the two? It has
been shown~\cite{cnn:em, BoxSup, ConsCNN} that a mixture of `strong'
supervision and `weak' supervision yields results almost as good as
those by pure `strong' supervision. Draw\&Tell provides an efficient
tool to create `weak' annotations.

\textbf{Training Time}: Another factor that matters is the training
time. For most of existing methods trained with image-level
supervision~\cite{img:pix:cnn, cnn:mil} and object
bounding-boxes~\cite{cnn:em, BoxSup}, an EM-style algorithm is needed,
meaning that the training (fine-tuning) of CNNs is performed for every
iteration. This increases the training time. HFCN, however, only needs
one round of training.

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.95\linewidth]{fig6/anno_cost_mIoU.pdf}
  \caption{Annotation vs. segmentation performance (mIoU) of all the
    methods considered. Evaluated on PASCAL VOC 2012 test.}
\label{fig:6}
\end{figure}
\section{Conclusion}
\label{sec:con}
In this paper, we developed an annotation method Draw\&Tell to create
training data for semantic image segmentation. Draw\&Tell allows
annotators to simply draw scribbles (strokes) on objects and speak
their names in the meanwhile, solving the \emph{what} and \emph{where}
problems once at the same time. We have proven experimentally that
Draw\&Tell is faster than other annotation methods, \eg $13$ times
faster than full-mask annotation, $4$ times faster than bounding-box
annotations, and $1.2$ times than the image-level annotation on the
PASCAL VOC dataset. Furthermore, we proposed a method to convert the
annotated scribbles to semantic heatmaps of corresponding classes, and
then extended the standard CNNs models to accommodate the soft heatmaps
as training data. We showed that the new CNNs method trained with the
heatmaps, yields significantly better results than competing methods
trained with supervision of comparable annotation cost, and yields comparable
results to the methods trained with significantly more expensive
annotations.  Introducing speech recognition to visual annotation is
helpful and this work is just a very first step in this direction. 

\textbf{Reproducibility}: The
annotations, code, and  trained models will be released upon the acceptance of the paper. 

% limitation: draw multiple times for the same objects to handle parts .
% how to handle fine-grained annotation
%-------------------------------------------------------------------------
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}




