\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{lazebnik:cvpr06,SVM-KNN,Bosch:iccv07,Boiman_CVPR_2008,Indoor,siftllc:cvpr10,Sun_2010,Yang_2014_CVPR}
\citation{game:purpose}
\citation{labelme}
\citation{Fergus09,Guillaumin:cvpr:10,dai:iccv13b}
\citation{JainK:cvpr09,cvpr09:multi:al}
\citation{Transfer:CVPR:08,tl:survey}
\citation{cvpr12:weak:video,metric:imitation}
\citation{self-taught:icml07}
\citation{Sivic05b,dai}
\citation{Fergus09,SemiForest,SemiBoost,Zhu:ISL:2009,book06:ssl,eccv10:ssl,ecml14:ssl,nips14:ssl}
\citation{Joachims:1999,neverhurt:icml11}
\citation{Zhu:Harmonic:03,Zhou:nips:04,Fergus09,icml10:large:graph:ssl}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction\relax }{section.1}{}}
\citation{SemiSVM,Joachims:1999,SemiBoost,SemiForest}
\citation{Rosch:1978}
\citation{Transfer:CVPR:08}
\citation{Rosch:1978}
\citation{EnClasReview}
\citation{Rosch:1978}
\citation{deep:bmvc14}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivations}{2}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Contributions}{2}{subsection.1.2}}
\citation{dai:iccv13b}
\citation{dai:eccv12b}
\citation{Ojala02}
\citation{gist}
\citation{Bosch:iccv07}
\citation{nips12:cnn,caffe14,rich:feature:cvpr14,deep:bmvc14}
\citation{dai:iccv13b}
\citation{dai:eccv12b}
\citation{random:hashing}
\citation{book06:ssl,Zhu:ISL:2009}
\citation{co-training:98}
\citation{Guillaumin:cvpr:10}
\citation{Semi:eccv12}
\citation{Guillaumin:cvpr:10}
\citation{Semi:eccv12}
\citation{Zhu:Harmonic:03}
\citation{Zhou:nips:04}
\citation{Belkin:semiframe:2006}
\citation{Fergus09}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The pipeline of Ensemble Projection (EP). EP consists of unsupervised feature learning (left panel) and plain classification or clustering (right panel). For feature learning, EP samples an ensemble of $T$ diverse prototype sets from all known images and learns discriminative classifiers on them for the projection functions. Images are then projected using these functions to obtain their new representation. These features are fed into standard classifiers and clustering methods for image classification and clustering respectively.}}{3}{figure.1}}
\newlabel{fig:pipeline}{{1}{3}{The pipeline of Ensemble Projection (EP). EP consists of unsupervised feature learning (left panel) and plain classification or clustering (right panel). For feature learning, EP samples an ensemble of $T$ diverse prototype sets from all known images and learns discriminative classifiers on them for the projection functions. Images are then projected using these functions to obtain their new representation. These features are fed into standard classifiers and clustering methods for image classification and clustering respectively}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}}
\newlabel{sec:related}{{2}{3}{Related Work\relax }{section.2}{}}
\citation{Joachims:1999}
\citation{SemiSVM}
\citation{SemiForest}
\citation{Zhu:ISL:2009}
\citation{SemiBoost}
\citation{SemiForest}
\citation{ensemble:iccv11}
\citation{ObjectAttribute:cvpr09}
\citation{eccv10:classemes}
\citation{li:objectbank}
\citation{augmented_attribute:eccv12,design_attribute:cvpr13}
\citation{stl-10}
\citation{cnnfet14}
\citation{mid-level:patches}
\citation{Fergus03}
\citation{Sivic05b}
\citation{Sivic08}
\citation{dai}
\citation{Frey_AffinityPropagation}
\citation{Frey_Dueck_2007}
\citation{Grauman06}
\citation{Tuytelaars_UnsupervisedSurvey}
\citation{gist}
\citation{Bosch:iccv07}
\citation{Ojala02}
\citation{deep:bmvc14}
\@writefile{toc}{\contentsline {section}{\numberline {3}Observations}{4}{section.3}}
\newlabel{sec:observations}{{3}{4}{Observations\relax }{section.3}{}}
\newlabel{sec:observation}{{3}{4}{Observations\relax }{section.3}{}}
\citation{lazebnik:cvpr06}
\citation{event-8}
\citation{lazebnik:cvpr06}
\citation{event-8}
\citation{lazebnik:cvpr06}
\citation{deep:bmvc14}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Observation 1}{5}{subsection.3.1}}
\newlabel{sec:mov1}{{3.1}{5}{Observation 1\relax }{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Observation 2}{5}{subsection.3.2}}
\newlabel{sec:mov2}{{3.2}{5}{Observation 2\relax }{subsection.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Our Approach}{5}{section.4}}
\newlabel{sec:approach}{{4}{5}{Our Approach\relax }{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The label co-occurrence probability $p(k)$: frequency of images having the same label with their $k_{th}$ neighbors. Results on six datasets are shown. The number of classes and the number of images of the datasets are shown as well. }}{6}{figure.2}}
\newlabel{fig:mov1}{{2}{6}{The label co-occurrence probability $p(k)$: frequency of images having the same label with their $k_{th}$ neighbors. Results on six datasets are shown. The number of classes and the number of images of the datasets are shown as well}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Classification accuracy of ensemble learning on the the Scene-15 dataset\nobreakspace  {}\citep  {lazebnik:cvpr06} and the Event-8 dataset\nobreakspace  {}\citep  {event-8}, for varying training label noise $R$ and varying number of training trials $T$. Experiments on other datasets obtain the same trend. Ensemble learning is able to cancell out the deficiency of the training sets even it is very severe (e.g. $R=80\%$), given that the deficiency modes are different or `orthogonal' and the number of training sets are sufficiently large. The figure is best viewed in color.}}{6}{figure.3}}
\newlabel{fig:mov2}{{3}{6}{Classification accuracy of ensemble learning on the the Scene-15 dataset~\citep {lazebnik:cvpr06} and the Event-8 dataset~\citep {event-8}, for varying training label noise $R$ and varying number of training trials $T$. Experiments on other datasets obtain the same trend. Ensemble learning is able to cancell out the deficiency of the training sets even it is very severe (e.g. $R=80\%$), given that the deficiency modes are different or `orthogonal' and the number of training sets are sufficiently large. The figure is best viewed in color}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Max-Min Sampling}{6}{subsection.4.1}}
\newlabel{sec:max-min}{{4.1}{6}{Max-Min Sampling\relax }{subsection.4.1}{}}
\citation{UIUC:Texture}
\citation{FeiFei2004}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Ensemble Projection}{7}{subsection.4.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Max-Min Sampling in $t^{th}$ trial}}{7}{algocf.1}}
\newlabel{alg:max-min}{{1}{7}{Ensemble Projection\relax }{algocf.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Ensemble Projection }}{7}{algocf.2}}
\newlabel{alg:ensemble:projection}{{2}{7}{Ensemble Projection\relax }{algocf.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{7}{section.5}}
\newlabel{sec:experiments}{{5}{7}{Experiments\relax }{section.5}{}}
\citation{stl-10}
\citation{lazebnik:cvpr06}
\citation{Indoor}
\citation{event-8}
\citation{building-25}
\citation{landuse21}
\citation{dai:eccv12b,dai:iccv13b}
\citation{gist}
\citation{Bosch:iccv07}
\citation{Ojala02}
\citation{caffe14,deep:bmvc14}
\citation{nips12:cnn,cnnfet14}
\citation{MatConvNet}
\citation{siftllc:cvpr10}
\citation{Zhu:Harmonic:03}
\citation{Belkin:semiframe:2006}
\citation{icml10:large:graph:ssl}
\citation{liblinear}
\citation{libsvm}
\citation{Belkin:semiframe:2006}
\citation{selftuning:04}
\citation{icml10:large:graph:ssl}
\citation{Zhu:Harmonic:03,Zhou:nips:04,icml10:large:graph:ssl,Fergus09,eccv10:ssl,ecml14:ssl}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Semi-supervised Image Classification}{8}{subsection.5.1}}
\newlabel{sec:sic}{{5.1}{8}{Semi-supervised Image Classification\relax }{subsection.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Classification results of Ensemble Projection (EP) on the eight datasets, where three classifiers are used: $k$-NN, Logistic Regression, and SVMs with RBF kernels. All methods were tested with two feature inputs: the original deep feature and the learned feature by EP on top of it (indicated by ``+ EP"). }}{9}{figure.4}}
\newlabel{fig:results:baseline}{{4}{9}{Classification results of Ensemble Projection (EP) on the eight datasets, where three classifiers are used: $k$-NN, Logistic Regression, and SVMs with RBF kernels. All methods were tested with two feature inputs: the original deep feature and the learned feature by EP on top of it (indicated by ``+ EP")}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Classification results of Ensemble Projection (EP) on the eight datasets, where three classifiers are used: $k$-NN, Logistic Regression, and SVMs with RBF kernels. All methods were tested with two feature inputs: the original deep feature and the learned feature by EP on top of it (indicated by ``+ EP"). }}{9}{figure.5}}
\newlabel{fig:results:ssl}{{5}{9}{Classification results of Ensemble Projection (EP) on the eight datasets, where three classifiers are used: $k$-NN, Logistic Regression, and SVMs with RBF kernels. All methods were tested with two feature inputs: the original deep feature and the learned feature by EP on top of it (indicated by ``+ EP")}{figure.5}{}}
\citation{Belkin:semiframe:2006}
\citation{nips14:ssl}
\citation{ObjectAttribute:cvpr09,Transfer:CVPR:08}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Precision (\%) of image classification on the eight datasets, with $5$ labeled training examples per class. ``+ EP" indicate that classifiers working with our learned feature as input rather than the original CNN. The best performance is indicated in \textbf  {bold}, and the second best is \relax $\@@underline {\hbox {underlined}}\mathsurround \z@ $\relax . }}{10}{table.1}}
\newlabel{table:precision}{{1}{10}{Precision (\%) of image classification on the eight datasets, with $5$ labeled training examples per class. ``+ EP" indicate that classifiers working with our learned feature as input rather than the original CNN. The best performance is indicated in \textbf {bold}, and the second best is \underline {underlined}}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Robustness to Parameters}{10}{subsubsection.5.1.1}}
\newlabel{sec:parameters}{{5.1.1}{10}{Robustness to Parameters\relax }{subsubsection.5.1.1}{}}
\citation{transformation:cvpr14}
\citation{cnnfet14}
\citation{deep:bmvc14}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  MAP (\%) of semi-supervised classification on the eight datasets, with $5$ labeled training examples per class. ``LR + EP" indicate Logistic Regression with our learned feature as input. The other two classifiers use the original CNN feature as input. The best number is indicated in \textbf  {bold}. }}{11}{table.2}}
\newlabel{table:map}{{2}{11}{MAP (\%) of semi-supervised classification on the eight datasets, with $5$ labeled training examples per class. ``LR + EP" indicate Logistic Regression with our learned feature as input. The other two classifiers use the original CNN feature as input. The best number is indicated in \textbf {bold}}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Performance of EP as a function its parameters $T$, $r$, $n$, and $m$, where LR is employed with $5$ labeled training images per class.}}{11}{figure.6}}
\newlabel{fig:mrnt}{{6}{11}{Performance of EP as a function its parameters $T$, $r$, $n$, and $m$, where LR is employed with $5$ labeled training images per class}{figure.6}{}}
\citation{neverhurt:icml11}
\citation{deep:bmvc14}
\citation{deep:bmvc14}
\citation{Sivic05b,Tuytelaars_UnsupervisedSurvey,dai,dai:eccv12b,fakton:eccv12}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of our learned feature to the CNN feature\nobreakspace  {}\cite  {deep:bmvc14}, with different LR models.}}{12}{figure.7}}
\newlabel{fig:classifiers}{{7}{12}{Comparison of our learned feature to the CNN feature~\cite {deep:bmvc14}, with different LR models}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Robustness to Classifier Models}{12}{subsubsection.5.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Efficiency}{12}{subsubsection.5.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Self-taught Image Classification}{12}{subsection.5.2}}
\newlabel{sec:self}{{5.2}{12}{Self-taught Image Classification\relax }{subsection.5.2}{}}
\citation{parallel:sc}
\citation{vlfeat}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Self-taught classification results on dataset STL-10, where EP is learned from the unlabeled images. The classifiers were tested with deep features, and our learned feature from it (indicated by ``+ EP").}}{13}{figure.8}}
\newlabel{fig:stl}{{8}{13}{Self-taught classification results on dataset STL-10, where EP is learned from the unlabeled images. The classifiers were tested with deep features, and our learned feature from it (indicated by ``+ EP")}{figure.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Purity (\%) of image clustering on the eight datasets, where the CNN feature \citep  {deep:bmvc14} and our learned feature from it (indicated by + EP) are used. The best results are indicated in \textbf  {bold}, and the second best is \relax $\@@underline {\hbox {underlined}}\mathsurround \z@ $\relax .}}{13}{table.3}}
\newlabel{table:clustering}{{3}{13}{Purity (\%) of image clustering on the eight datasets, where the CNN feature \citep {deep:bmvc14} and our learned feature from it (indicated by + EP) are used. The best results are indicated in \textbf {bold}, and the second best is \underline {underlined}}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Image Clustering}{13}{subsection.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{13}{section.6}}
\newlabel{sec:conclusion}{{6}{13}{Conclusion\relax }{section.6}{}}
\bibstyle{spbasic}
\bibdata{egbib}
\bibcite{Bosch:iccv07}{{1}{????}{{Bos}}{{}}}
\bibcite{Belkin:semiframe:2006}{{2}{2006}{{Belkin et~al}}{{Belkin, Niyogi, and Sindhwani}}}
\bibcite{SemiSVM}{{3}{1998}{{Bennett and Demiriz}}{{}}}
\bibcite{co-training:98}{{4}{1998}{{Blum and Mitchell}}{{}}}
\bibcite{Boiman_CVPR_2008}{{5}{2008}{{Boiman et~al}}{{Boiman, Shechtman, and Irani}}}
\bibcite{libsvm}{{6}{2011}{{Chang and Lin}}{{}}}
\bibcite{book06:ssl}{{7}{2006}{{Chapelle et~al}}{{Chapelle, Sch{\"o}lkopf, and Zien}}}
\bibcite{deep:bmvc14}{{8}{2014}{{Chatfield et~al}}{{Chatfield, Simonyan, Vedaldi, and Zisserman}}}
\bibcite{parallel:sc}{{9}{2011}{{Chen et~al}}{{Chen, Song, Bai, Lin, and Chang}}}
\bibcite{stl-10}{{10}{2011}{{Coates et~al}}{{Coates, Ng, and Lee}}}
\bibcite{dai:iccv13b}{{11}{2013}{{Dai and {Van Gool}}}{{}}}
\bibcite{dai}{{12}{2010}{{Dai et~al}}{{Dai, Wu, and Zhu}}}
\bibcite{dai:eccv12b}{{13}{2012}{{Dai et~al}}{{Dai, Prasad, Leistner, and Gool}}}
\bibcite{metric:imitation}{{14}{2015}{{Dai et~al}}{{Dai, Kroeger, Timofte, and {Van Gool}}}}
\bibcite{caffe14}{{15}{2014}{{Donahue et~al}}{{Donahue, Jia, Vinyals, Hoffman, Zhang, Tzeng, and Darrell}}}
\bibcite{cnnfet14}{{16}{2014}{{Dosovitskiy et~al}}{{Dosovitskiy, {T. Springenberg}, Riedmiller, and Brox}}}
\bibcite{Frey_AffinityPropagation}{{17}{2007}{{Dueck and Frey}}{{}}}
\bibcite{eccv10:ssl}{{18}{2010}{{Ebert et~al}}{{Ebert, Larlus, and Schiele}}}
\bibcite{fakton:eccv12}{{19}{2012}{{Faktor and Irani}}{{}}}
\bibcite{liblinear}{{20}{2008}{{Fan et~al}}{{Fan, Chang, Hsieh, Wang, and Lin}}}
\bibcite{ObjectAttribute:cvpr09}{{21}{2009}{{Farhadi et~al}}{{Farhadi, Endres, Hoiem, and Forsyth}}}
\bibcite{FeiFei2004}{{22}{2004}{{Fei-Fei et~al}}{{Fei-Fei, Fergus, and Perona}}}
\bibcite{Fergus03}{{23}{2003}{{Fergus et~al}}{{Fergus, Perona, and Zisserman}}}
\bibcite{Fergus09}{{24}{2009}{{Fergus et~al}}{{Fergus, Weiss, and Torralba}}}
\bibcite{Frey_Dueck_2007}{{25}{2007}{{Frey and Dueck}}{{}}}
\bibcite{rich:feature:cvpr14}{{26}{2014}{{Girshick et~al}}{{Girshick, Donahue, Darrell, and Malik}}}
\bibcite{Grauman06}{{27}{2006}{{Grauman and Darrell}}{{}}}
\bibcite{Guillaumin:cvpr:10}{{28}{2010}{{Guillaumin et~al}}{{Guillaumin, Verbeek, and Schmid}}}
\bibcite{JainK:cvpr09}{{29}{2009}{{Jain and Kapoor}}{{}}}
\bibcite{Joachims:1999}{{30}{1999}{{Joachims}}{{}}}
\bibcite{cvpr09:multi:al}{{31}{2009}{{Joshi et~al}}{{Joshi, Porikli, and Papanikolopoulos}}}
\bibcite{nips14:ssl}{{32}{2014}{{Kingma et~al}}{{Kingma, Mohamed, Rezende, and Welling}}}
\bibcite{ensemble:iccv11}{{33}{2011}{{Kozakaya et~al}}{{Kozakaya, Ito, and Kubota}}}
\bibcite{nips12:cnn}{{34}{2012}{{Krizhevsky et~al}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{SemiBoost}{{35}{2009}{{Kumar~Mallapragada et~al}}{{Kumar~Mallapragada, Jin, Jain, and Liu}}}
\bibcite{UIUC:Texture}{{36}{2005}{{Lazebnik et~al}}{{Lazebnik, Schmid, and Ponce}}}
\bibcite{lazebnik:cvpr06}{{37}{2006}{{Lazebnik et~al}}{{Lazebnik, Schmid, and Ponce}}}
\bibcite{SemiForest}{{38}{2009}{{Leistner et~al}}{{Leistner, Saffari, Santner, and Bischof}}}
\bibcite{event-8}{{39}{2007}{{Li and Fei-Fei}}{{}}}
\bibcite{li:objectbank}{{40}{2010}{{Li et~al}}{{Li, Su, Xing, and Li}}}
\bibcite{neverhurt:icml11}{{41}{2011}{{Li and Zhou}}{{}}}
\bibcite{icml10:large:graph:ssl}{{42}{2010}{{Liu et~al}}{{Liu, He, and Chang}}}
\bibcite{Ojala02}{{43}{2002}{{Ojala et~al}}{{Ojala, Pietik\"{a}inen, and M\"{a}enp\"{a}\"{a}}}}
\bibcite{gist}{{44}{2001}{{Oliva and Torralba}}{{}}}
\bibcite{tl:survey}{{45}{2010}{{Pan and Yang}}{{}}}
\bibcite{transformation:cvpr14}{{46}{2014}{{Paulin et~al}}{{Paulin, Revaud, Harchaoui, Perronnin, and Schmid}}}
\bibcite{ecml14:ssl}{{47}{2014}{{Pitelis et~al}}{{Pitelis, Russell, and Agapito}}}
\bibcite{cvpr12:weak:video}{{48}{2012}{{Prest et~al}}{{Prest, Leistner, Civera, Schmid, and Ferrari}}}
\bibcite{Indoor}{{49}{2009}{{Quattoni and Torralba}}{{}}}
\bibcite{Transfer:CVPR:08}{{50}{2008}{{Quattoni et~al}}{{Quattoni, Collins, and Darrell}}}
\bibcite{self-taught:icml07}{{51}{2007}{{Raina et~al}}{{Raina, Battle, Lee, Packer, and Ng}}}
\bibcite{EnClasReview}{{52}{2010}{{Rokach}}{{}}}
\bibcite{Rosch:1978}{{53}{1978}{{Rosch}}{{}}}
\bibcite{labelme}{{54}{2008}{{Russell et~al}}{{Russell, Torralba, Murphy, and Freeman}}}
\bibcite{augmented_attribute:eccv12}{{55}{2012}{{Sharmanska et~al}}{{Sharmanska, Quadrianto, and Lampert}}}
\bibcite{Semi:eccv12}{{56}{2012}{{Shrivastava et~al}}{{Shrivastava, Singh, and Gupta}}}
\bibcite{mid-level:patches}{{57}{2012}{{Singh et~al}}{{Singh, Gupta, and Efros}}}
\bibcite{Sivic05b}{{58}{2005}{{Sivic et~al}}{{Sivic, Russell, Efros, Zisserman, and Freeman}}}
\bibcite{Sivic08}{{59}{2008}{{Sivic et~al}}{{Sivic, Russell, Zisserman, Freeman, and Efros}}}
\bibcite{random:hashing}{{60}{2013}{{Tang et~al}}{{Tang, Tzu-Wei, and Chen}}}
\bibcite{eccv10:classemes}{{61}{2010}{{Torresani et~al}}{{Torresani, Szummer, and Fitzgibbon}}}
\bibcite{Tuytelaars_UnsupervisedSurvey}{{62}{2009}{{Tuytelaars et~al}}{{Tuytelaars, Lampert, Blaschko, and Buntine}}}
\bibcite{vlfeat}{{63}{2008}{{Vedaldi and Fulkerson}}{{}}}
\bibcite{MatConvNet}{{64}{2014}{{Vedaldi and Lenc}}{{}}}
\bibcite{game:purpose}{{65}{2006}{{Von~Ahn}}{{}}}
\bibcite{siftllc:cvpr10}{{66}{2010}{{Wang et~al}}{{Wang, Yang, Yu, Lv, Huang, and Gong}}}
\bibcite{Sun_2010}{{67}{2010}{{Xiao et~al}}{{Xiao, Hays, Ehinger, Oliva, and Torralba}}}
\bibcite{building-25}{{68}{2014}{{Xu et~al}}{{Xu, Tao, Zhang, Wu, and Tsoi}}}
\bibcite{Yang_2014_CVPR}{{69}{2014}{{Yang et~al}}{{Yang, Dai, Shen, and Gool}}}
\bibcite{landuse21}{{70}{2010}{{Yang and Newsam}}{{}}}
\bibcite{design_attribute:cvpr13}{{71}{2013}{{Yu et~al}}{{Yu, Cao, Feris, Smith, and Chang}}}
\bibcite{selftuning:04}{{72}{2004}{{Zelnik-Manor and Perona}}{{}}}
\bibcite{SVM-KNN}{{73}{2006}{{Zhan et~al}}{{Zhan, Berg, Maire, and Malik}}}
\bibcite{Zhou:nips:04}{{74}{2004}{{Zhou et~al}}{{Zhou, Bousquet, Lal, Weston, and Schölkopf}}}
\bibcite{Zhu:ISL:2009}{{75}{2009}{{Zhu and Goldberg}}{{}}}
\bibcite{Zhu:Harmonic:03}{{76}{2003}{{Zhu et~al}}{{Zhu, Ghahramani, and Lafferty}}}
