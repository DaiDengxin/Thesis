\chapter{Introduction}

A picture is worth a thousand of words, and often comes with a story
that the photographer wants to deliver. We probably still remember
the moments when our grandparents were telling us the stories behind the
old pictures at our home, be it about birthday parties, be it about
holidays. The purpose of taking pictures has been extended
dramatically in the last two decades, due to the invention of the Internet and the wide 
deployment of digital cameras.  Nowadays, it is effortless ever to take pictures, store them, and share them with friends and
families.  While images and videos are still used to remember special moments of our daily life,  they have been widely used for many other practical applications, such as for security in public places, for product control in factories, for
product description in E-commerce, for real-world objects reconstruction, for robot control, to name a few. 
%This leads to an outright explosion in the number of images and videos taken, which we will not ever have enough time and effort to read through.

\section{Motivation}
%Taken YouTube as an example, it has 100 hours of videos (100 million frames) updated every minute. 
The potential of using visual data (images and videos) is huge,  and the quality and quantity of the data are
steadily increasing.  However,  the full potential can hardly be reached if they are unorganized. In computer vision,
algorithms are developed to tackle the fundamental problems of processing and understanding the visual data, such as image classification, object detection, and object tracking. 
With such algorithms, images and videos can then be
converted into semantic, symbolic or numeric information to better suit the needs of practical applications. 

%A plethora of methods have been developed to tackle each of the vision tasks, and great advances have been made in the past years. To the time being, computer vision can already recognize objects of thousands of categories, detecting objects in images in real time, and can already reconstruct 3D models at city- or world-scale.  Despite the great achievement,  the field still has to find means to keep up with the exploration of the massive amounts of data being captured on a daily basis. We believe this is mainly due to the lack of sufficient training annotations and the lack of computational resources. The thesis is dedicated to mitigate the problem.

The last decade has witnessed a great progress in object recognition, which comes as the outcome of three factors: 1)  large-scale human annotations, such as ImageNet \citep{imagenet} for image recognition and detection, PASCAL VOC \citep{pascal:2011} and MSCOCO \citep{coco:eccv} for object detection and segmentation, KITTI \citep{kitti} and Cityscapes \citep{cityscapes} for road scene understanding; 2) sophisticated, statistical learning approaches including Support Vector Machines, Adaboost, Random Forests, and Deep Neural Networks; and 3)  powerful computing infrastructures such as the GPUs and many software frameworks such as CuNN, Caffe and Torch. These advantages help the field achieved multiple milestones: computer vision now is able to recognize objects of thousands of categories in high accuracy \citep{deepnet:nips12, vgg16, ResNet}, transcribe images/videos directly into natural languages \citep{show:tell:caption}, and generate realistic images \citep{draw:2015}. 

Despite the great achievement, the field still has to find means to keep up with the exploration of massive amount of visual data.  Visual content is exploding -- millions of videos are generated every second, from the footage of surveillance cameras to the over two billion images and videos uploaded daily to social platforms. However, intelligent and insightful understanding of all these data is still far from being reached.  There are at least three reasons behind this: 1) visual annotation is expensive to obtain; 2)  algorithms in computer vision are often computationally heavy; and 3) developed methods are not performance-aware most of time.   

The first reason is evidenced by the fact that top-performing recognition methods are all trained in a supervised manner. However, the speed of visual annotation can hardly match that of visual data acquisition --- taking pictures, storing and sharing them are effortless nowadays, while annotating visual data in the form that machine can learn on is tedious and very time-intensive. 
For example, 
%On one hand, an explosive amount of visual data is captured everyday and needs to be analyzed. 
%be it from surveillance cameras for security reasons, be it from industrial or mobile robot for controlling processes, be it the internet images uploaded by casual users due to social activities. 
YouTube has 100 hours of videos (100 million frames) updated every minute, while it required 19 man-years to label 1.2 million internet images \citep{imagenet}. The annotations are done in the form of bounding boxes, rather than in more dedicated forms such as full segmentation masks.  This also explains why most of the existing benchmarks still only cover a small portion of the visual data  available in the wild, and were collected in quite controlled scenarios. 

The second reason follows the fact that visual data is very high-dimensional,  rendering the algorithms computationally heavy in both training and testing. For instance, training a state-of-the-art image classifier \citep{vgg16, ResNet} can take days or weeks even with modern GPUs,  computing optical flow for a standard video dataset \citep{large:video:cnn} can simply take several days,  and computing the pairwise similarity among images of a large dataset can be prohibitively expensive as well \citep{Gong_2013_CVPR}. This heavy computation hinders the community from quickly exploring new models,  easily deploying the trained models to power-limited devices, and readily scaling the developed models. 

The last one is due to the trend of the community -- people are mainly focused on developing the next best method for task X, and not much on improving the performance-awareness of the methods, be it  a measure of model uncertainty \citep{kendall2015bayesian, confidence:of:08},  be it the usefulness of the methods to down-streamed vision tasks \citep{can-similar-scene-help, eth_biwi_00883,  15000object}. This performance blindness leads to ineffective solutions in many situations, such as the same amount of resource is allocated to every image regardless of its complexity (difficulty), and the advance made in all the sub-fields of computer vision cannot be synergized effectively.  

\section{Contributions}
The aforementioned problems deliver a strong need to reduce the annotation cost and computational cost of current computer vision methods, and to learn to predict their performance.  This thesis is dedicated to provide a collection of methods to attack these problems from multiple perspectives. We mainly focus on recognition-relevant  tasks.  

First, we elaborate two strategies to reduce the annotation cost: 
\begin{itemize}
\item  Developing efficient visual annotation approaches.  A natural and efficient annotation method is developed for object recognition by letting annotators speak.  Since drawing scribbles and speaking are very natural to human,  our method unleashes the expressive ability of annotators and solves the \emph{what} and \emph{where} problems of object annotation both at the same time. 
Combining speaking and drawing leads to an efficient annotation approach.  

\item  Learning  feature representation with unlabeled data.  A novel method is developed to learn a new feature representation on top of standard feature representations.  The leaning takes advantage of discriminative learning and ensemble learning, and is able to generate new features specifically tailored to the data at hand.  
\end{itemize}

Secondly, we examine how to reduce the computational cost associated with the training and testing of recognition algorithms in the context of \emph{Leaning with Privileged  Information} and \emph{Metric Learning}.   
\begin{itemize}
\item Developing fast training algorithms for SVM+ \citep{SVMplus_vapnik}.   SVM+ has shown excellent performance in visual recognition tasks for exploiting privileged information of the
  training data.   We propose two efficient algorithms
  for solving the linear and kernel SVM+. 
    Experiments show that our proposed algorithms achieve significant
  speed-ups to  the state-of-the-art solvers for  SVM+.
 

\item Proposing Metric Imitation (MI), a method to allow computationally cheap features to imitate alternative features which perform better but are computationally more expensive.   The leaned transformation significantly boost the performance of cheap features while retaining their efficiency.    
\end{itemize}

Lastly, we investigate performance-aware vision algorithms by making the following contributions:  
\begin{itemize}
\item  Predicting \emph{Succeed} or \emph {Fail}  for an algorithm on particular samples. 
 It is generally true that not all images are equally difficulty. We learn this property for the task of example-based texture synthesis~(ETS). ETS has been widely used to
  generate high quality textures of desired sizes from a small
  example. However, not all textures are equally well reproducible
  that way. We predict how synthesizable a particular texture
  is by ETS and find that texture synthesizability can be learned and predicted efficiently. 

\item  Predicting \emph{Under-}, \emph{Properly-} or \emph{Over-Performed} for an algorithm on particular samples. The prediction is examined on image segmentation.  Hierarchical image segmentation provides segmentation at different scales in a single tree-like structure. However, they are not aware of the scale information of the regions in them.
As such, one might need to work on many different levels of the hierarchy to find the objects in the scene.
This work predicts the  scales of the regions relative to the size of corresponding objects, which signals  \emph{under-}, \emph{properly-}, or \emph{over-segmented}.  The prediction is then used to better couple tree depth  and region scale.
The output of the method is an improved hierarchy, which improves the quality of the hierarchical segmentation representations.
  
\item  Evaluating the usefulness of one task for other tasks. We examined this on image super-resolution for another four popular vision tasks.  Although it might be believed that image super-resolution is helpful for other vision tasks, this work has formalized the conception and conducted quantitative evaluation.
The work can serve as an inspiration for the community  to evaluate image super-resolution with respect to the helpfulness to other vision tasks, and to apply it as a pre-processing component if the input images are of low-resolution. 

\end{itemize}


\section{Organization} 
The thesis begins with Chapter~\ref{ch:relatedwork} examining related work in a broad context.   Our developed approaches are presented in Chapters \ref{ch:draw-and-tell} -- \ref{ch:SR4VT}. They are written to be generally self-contained and can be read independently. Finally,  Chapter~\ref{ch:conclusion} concludes this thesis. A detailed overview of the remaining chapters follows:

In Chapter~\ref{ch:relatedwork}, \emph{Related Work}, we provide a short literature overview of previous art in the direction of  reducing the annotation cost and computational cost of vision algorithms and towards performance-aware vision algorithms. 

In Chapter~\ref{ch:draw-and-tell}, \emph{Efficient Visual Annotation with Speech Recognition}, we present our efficient visual annotation approach Draw\&Tell and show its efficiency in the context of semantic image segmentation. In order to solve the \emph{what} and \emph{where} problems in visual annotation both at the same time, we let annotators speak the name of the object  while they draw strokes on it.  The speech is recognized by a speech recognition engine specifically trained for the purpose, and an extension to the fully convolutional neural network is made to learn from the stroke-based `coarse' annotation.  The approach draws a good trade-off between recognition accuracy and annotation cost.  The work was originally presented in \citep{draw:tell}. 

In Chapter~\ref{ch:ensemble}, \emph{Representation Learning with Unlabeled Data}, we present our motivation and method of learning a new feature representation with unlabeled data.  We then evaluate the method in the context of semi-supervised image classification and image clustering, and show that they outperforms competing methods.    This work was originally presented in \citep{dai:eccv12b} and in \citep{dai:iccv13b}. 


In Chapter~\ref{ch:svmplus}, \emph{Fast Training Algorithms for SVM+}, we present two efficient algorithms
  for training the linear and kernel SVM+.  New problems with fewer constraints are formulated in the dual domain, making the problem solvable  efficiently by the SMO algorithm of one-class SVM.  Experiments show that our proposed algorithms are significantly faster than the  the state-of-the-art solvers for SVM+.    This work was originally presented in \citep{fastsvm+2016}. 
  
  
 In Chapter~\ref{ch:mi}, \emph{Efficient Metric Computation via Imitation}, we present a method called Metric Imitation (MI) to efficiently compute the distance among images.  MI learns a transformation to cheap features so that the distance with the transformed features can approximate the distance with better-performing but computationally-expensive features.  
The method was originally presented in \citep{metric:imitation}. 
 
In Chapter~\ref{ch:forecasting}, \emph{Performance Prediction: Succeed or Fail?}, we present our approach of predicting the success of example-based texture synthesis.  To this aim, we collected a dataset with  $21,302$ annotated textures and annotated them  according to the synthesizability --- the quality of synthesized results by texture synthesis methods. A set of relevant features are then defined to regress the value of synthesizability. Extensive experiments show that texture synthesizability is learnable. 
This work was originally presented in \citep{dai:synthesizability}. 


In Chapter~\ref{ch:scale-aware}, \emph{Performance Prediction: Under-, Properly-, or Over-Processed?}, 
we present a method to predict the  scale of image segments relative to the corresponding objects, and to then apply the prediction to re-align the results from general hierarchical image segmentation so that the depth in the tree structure and the scale of the regions is better coupled. The output of our method is an improved hierarchy, which improves the quality of the hierarchical segmentation representations.  The work was originally presented in \citep{Chen2016}. 



In Chapter~\ref{ch:SR4VT}, \emph{Performance Evaluation: Helpful for Other Tasks?}, we evaluate the usefulness of image super-resolution methods to other computer vision tasks. Sixes state-of-the-art computer vision algorithms are evaluated on four popular computer vision tasks: boundary detection, semantic image segmentation, digit recognition, and scene recognition.  The work confirms that image super-resolution is helpful for other vision tasks when the state-of-the-art approaches are used. 
The work was originally presented in \citep{SR4VTs:wacv16}. 


 